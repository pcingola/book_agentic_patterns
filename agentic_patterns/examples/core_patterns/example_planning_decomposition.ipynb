{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Planning and Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_patterns.core.agents import get_agent, run_agent\n",
    "from agentic_patterns.core.agents.utils import nodes_to_message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_problem",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Build a data pipeline to analyze website traffic logs.\n",
    "\n",
    "This problem benefits from planning because:\n",
    "- It involves multiple distinct phases (data loading, cleaning, analysis, reporting)\n",
    "- Steps have dependencies (can't analyze before cleaning)\n",
    "- The solution structure matters as much as individual steps\n",
    "- An explicit plan can be reviewed before execution begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "problem_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"Build a data pipeline to analyze website traffic logs.\n",
    "\n",
    "Context:\n",
    "- Log files are in JSON format, one event per line\n",
    "- Each event has: timestamp, user_id, page_url, referrer, user_agent\n",
    "- Files are stored in an S3 bucket, organized by date (logs/YYYY/MM/DD/)\n",
    "- Need to produce a daily report showing:\n",
    "  * Total page views and unique visitors\n",
    "  * Top 10 most visited pages\n",
    "  * Traffic sources breakdown (direct, search, social, referral)\n",
    "  * Hourly traffic pattern\n",
    "\n",
    "Design and implement this pipeline.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step1",
   "metadata": {},
   "source": [
    "## Step 1: Generate the Plan\n",
    "\n",
    "Ask the model to create an explicit plan before any implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generate_plan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation Plan:\n",
      "Here's a detailed implementation plan for the website traffic logs analysis pipeline:\n",
      "\n",
      "1. Log File Collection & Validation\n",
      "   - Purpose: Gather daily log files and verify data integrity\n",
      "   - Inputs: Raw JSON log files from S3 (logs/YYYY/MM/DD/)\n",
      "   - Outputs: Validated JSON files, error report for malformed records\n",
      "   - Dependencies: None\n",
      "\n",
      "2. Data Parsing & Enrichment\n",
      "   - Purpose: Parse JSON, standardize fields, add derived data\n",
      "   - Inputs: Validated JSON files\n",
      "   - Outputs: Parquet files with enriched data including:\n",
      "     * Parsed timestamp with hour extraction\n",
      "     * Classified traffic source (using referrer analysis)\n",
      "     * Normalized URLs (removing query parameters)\n",
      "     * Parsed user agent information\n",
      "   - Dependencies: Step 1\n",
      "\n",
      "3. Daily Aggregation Processing\n",
      "   - Purpose: Calculate core metrics for the day\n",
      "   - Inputs: Enriched Parquet files\n",
      "   - Outputs: Four separate analytical tables:\n",
      "     * Daily visitor counts (total and unique)\n",
      "     * Page view counts by URL\n",
      "     * Traffic source distribution\n",
      "     * Hourly traffic counts\n",
      "   - Dependencies: Step 2\n",
      "\n",
      "4. Top Pages Analysis\n",
      "   - Purpose: Calculate top 10 most visited pages\n",
      "   - Inputs: Page view counts table\n",
      "   - Outputs: Ranked list of pages with view counts\n",
      "   - Dependencies: Step 3\n",
      "\n",
      "5. Traffic Pattern Analysis\n",
      "   - Purpose: Generate hourly traffic patterns\n",
      "   - Inputs: Hourly traffic counts\n",
      "   - Outputs: 24-hour traffic distribution data\n",
      "   - Dependencies: Step 3\n",
      "\n",
      "6. Report Generation\n",
      "   - Purpose: Compile all analyses into final report format\n",
      "   - Inputs: \n",
      "     * Daily visitor counts\n",
      "     * Top pages list\n",
      "     * Traffic source distribution\n",
      "     * Hourly pattern data\n",
      "   - Outputs: Daily report in both JSON and HTML formats\n",
      "   - Dependencies: Steps 4 and 5\n",
      "\n",
      "7. Data Storage & Archival\n",
      "   - Purpose: Store processed data and reports for historical access\n",
      "   - Inputs: \n",
      "     * Enriched Parquet files\n",
      "     * Generated reports\n",
      "     * Analytical tables\n",
      "   - Outputs: Archived data in appropriate storage locations\n",
      "   - Dependencies: Step 6\n",
      "\n",
      "8. Monitoring & Alerting Setup\n",
      "   - Purpose: Track pipeline health and data quality\n",
      "   - Inputs: \n",
      "     * Pipeline execution metrics\n",
      "     * Data quality metrics\n",
      "     * Processing logs\n",
      "   - Outputs:\n",
      "     * Pipeline status dashboard\n",
      "     * Alert notifications for anomalies\n",
      "   - Dependencies: All previous steps\n",
      "\n",
      "Infrastructure Recommendations:\n",
      "- Use Apache Airflow for orchestration\n",
      "- Process data using Apache Spark for scalability\n",
      "- Store processed data in Amazon Redshift or Snowflake\n",
      "- Use Amazon CloudWatch for monitoring\n",
      "- Store reports in S3 with appropriate lifecycle policies\n",
      "\n",
      "Error Handling Considerations:\n",
      "- Implement retry logic for failed tasks\n",
      "- Create error logs for data quality issues\n",
      "- Set up alerts for critical failures\n",
      "- Maintain partial success capability where appropriate\n",
      "\n",
      "This pipeline should be scheduled to run daily, processing the previous day's logs, with all steps executed sequentially based on their dependencies.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_planner = \"\"\"You are a software architect. Your job is to create implementation plans.\n",
    "Do NOT write code. Create a structured plan that a developer can follow.\"\"\"\n",
    "\n",
    "prompt_plan = f\"\"\"{task}\n",
    "\n",
    "Create a detailed implementation plan. For each step:\n",
    "1. Give it a clear name\n",
    "2. Describe what it accomplishes\n",
    "3. List inputs it requires\n",
    "4. List outputs it produces\n",
    "5. Note any dependencies on other steps\n",
    "\n",
    "Format as a numbered list. Do not write code.\"\"\"\n",
    "\n",
    "agent_planner = get_agent(config_name=\"fast\", system_prompt=system_prompt_planner)\n",
    "agent_run_plan, nodes_plan = await run_agent(agent_planner, prompt_plan)\n",
    "\n",
    "print(\"Implementation Plan:\")\n",
    "print(agent_run_plan.result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step2",
   "metadata": {},
   "source": [
    "## Step 2: Validate the Plan\n",
    "\n",
    "Before implementation, verify the plan is complete and feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "validate_plan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan Validation:\n",
      "Let me review the plan systematically:\n",
      "\n",
      "COMPLETENESS CHECK:\n",
      "✓ Page views and unique visitors (Step 3)\n",
      "✓ Top 10 pages (Step 4)\n",
      "✓ Traffic sources (Steps 2 & 3)\n",
      "✓ Hourly patterns (Steps 2 & 5)\n",
      "\n",
      "IDENTIFIED GAPS:\n",
      "\n",
      "1. Data Testing/Quality Assurance is underspecified\n",
      "2. No backup/recovery procedures\n",
      "3. Schema management not addressed\n",
      "4. Initial setup/deployment process missing\n",
      "\n",
      "Here's an updated plan with new steps (additions marked with *):\n",
      "\n",
      "*0. Initial Setup & Configuration\n",
      "   - Purpose: Configure infrastructure and define schemas\n",
      "   - Inputs: Configuration parameters, schema definitions\n",
      "   - Outputs: Deployed infrastructure, validated schemas\n",
      "   - Dependencies: None\n",
      "\n",
      "1-8. [Previous steps remain the same]\n",
      "\n",
      "*9. Data Quality Validation\n",
      "   - Purpose: Verify data completeness and accuracy\n",
      "   - Inputs: \n",
      "     * Raw and processed datasets\n",
      "     * Expected value ranges\n",
      "     * Historical patterns\n",
      "   - Outputs:\n",
      "     * Data quality scorecard\n",
      "     * Validation report\n",
      "     * Data quality metrics\n",
      "   - Dependencies: Steps 2, 3\n",
      "\n",
      "*10. Backup & Recovery\n",
      "   - Purpose: Ensure data recoverability\n",
      "   - Inputs:\n",
      "     * Processed data\n",
      "     * Reports\n",
      "     * Configuration\n",
      "   - Outputs:\n",
      "     * Backup copies\n",
      "     * Recovery point validation\n",
      "   - Dependencies: Step 7\n",
      "\n",
      "*11. Schema Management\n",
      "   - Purpose: Handle schema evolution and compatibility\n",
      "   - Inputs:\n",
      "     * Current schema\n",
      "     * Schema change requests\n",
      "   - Outputs:\n",
      "     * Schema version history\n",
      "     * Migration scripts\n",
      "     * Compatibility reports\n",
      "   - Dependencies: Step 0\n",
      "\n",
      "DEPENDENCY UPDATES:\n",
      "- Step 8 (Monitoring) should also depend on new Steps 9 and 10\n",
      "- Step 7 (Storage) should include schema version tracking\n",
      "\n",
      "ADDITIONAL SPECIFICATIONS:\n",
      "1. Data Parsing & Enrichment (Step 2) should include:\n",
      "   - URL normalization rules\n",
      "   - Traffic source classification criteria\n",
      "   - User agent parsing specifications\n",
      "\n",
      "2. Daily Aggregation (Step 3) should specify:\n",
      "   - Visitor deduplication logic\n",
      "   - Time zone handling\n",
      "   - Aggregation window definitions\n",
      "\n",
      "3. Monitoring (Step 8) should include:\n",
      "   - SLA definitions\n",
      "   - Alert thresholds\n",
      "   - Escalation procedures\n",
      "\n",
      "The updated plan is more comprehensive and addresses the identified gaps. Each new step provides necessary functionality for a production-grade system. The dependencies are properly ordered to ensure data quality and system reliability.\n",
      "\n",
      "Would you like me to detail any specific aspect of the updated plan further?\n"
     ]
    }
   ],
   "source": [
    "message_history = nodes_to_message_history(nodes_plan)\n",
    "\n",
    "prompt_validate = \"\"\"Review the plan you created. Check for:\n",
    "\n",
    "1. Completeness: Does the plan cover all requirements?\n",
    "2. Dependencies: Are step dependencies correctly ordered?\n",
    "3. Feasibility: Are there any steps that seem unclear or underspecified?\n",
    "4. Missing steps: Is anything needed that wasn't included?\n",
    "\n",
    "If issues are found, provide an updated plan. Otherwise confirm the plan is ready.\"\"\"\n",
    "\n",
    "agent_run_validate, nodes_validate = await run_agent(\n",
    "    agent_planner, prompt_validate, message_history=message_history\n",
    ")\n",
    "\n",
    "print(\"Plan Validation:\")\n",
    "print(agent_run_validate.result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step3",
   "metadata": {},
   "source": [
    "## Step 3: Decompose a Complex Step\n",
    "\n",
    "Some steps may need further decomposition. Here we break down a complex step into sub-tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decompose_step",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed Sub-Tasks:\n",
      "Here's a detailed breakdown of the Traffic Sources Classification step:\n",
      "\n",
      "SUB-TASKS:\n",
      "\n",
      "1. Referrer URL Parsing\n",
      "   - Extract domain from referrer URL\n",
      "   - Handle URL encoding/decoding\n",
      "   - Normalize domains (remove www., standardize format)\n",
      "   - Output: Clean referrer domain\n",
      "\n",
      "2. UTM Parameter Processing\n",
      "   - Extract utm_source\n",
      "   - Extract utm_medium\n",
      "   - Extract utm_campaign\n",
      "   - Handle URL-encoded parameters\n",
      "   - Output: Structured UTM data\n",
      "\n",
      "3. Search Engine Detection\n",
      "   - Match against search engine domain list\n",
      "   - Extract search query parameters\n",
      "   - Handle different search engine URL patterns\n",
      "   - Output: Boolean is_search_engine + engine_name\n",
      "\n",
      "4. Social Media Classification\n",
      "   ```python\n",
      "   social_platforms = {\n",
      "       'facebook.com': 'Facebook',\n",
      "       'twitter.com': 'Twitter',\n",
      "       't.co': 'Twitter',\n",
      "       'linkedin.com': 'LinkedIn',\n",
      "       'instagram.com': 'Instagram'\n",
      "       # etc.\n",
      "   }\n",
      "   ```\n",
      "   - Match against social domain list\n",
      "   - Handle short URLs (t.co, bit.ly, etc.)\n",
      "   - Output: Boolean is_social + platform_name\n",
      "\n",
      "5. Traffic Source Categorization\n",
      "   Primary categories:\n",
      "   ```python\n",
      "   CATEGORIES = {\n",
      "       'direct': {'conditions': ['null_referrer', 'same_domain']},\n",
      "       'organic_search': {'conditions': ['is_search_engine', 'no_utm']},\n",
      "       'paid_search': {'conditions': ['is_search_engine', 'utm_medium=cpc']},\n",
      "       'social': {'conditions': ['is_social']},\n",
      "       'email': {'conditions': ['utm_medium=email']},\n",
      "       'referral': {'conditions': ['has_referrer', 'not_search', 'not_social']},\n",
      "       'other': {'conditions': ['default']}\n",
      "   }\n",
      "   ```\n",
      "\n",
      "6. Direct Traffic Handling\n",
      "   - Identify true direct traffic (no referrer)\n",
      "   - Handle browser-stripped referrers\n",
      "   - Identify same-domain navigation\n",
      "   - Output: Boolean is_direct\n",
      "\n",
      "EDGE CASE HANDLING:\n",
      "\n",
      "1. Missing Referrer\n",
      "   ```python\n",
      "   def handle_missing_referrer(session_data):\n",
      "       if session_data.get('is_first_visit'):\n",
      "           return 'direct'\n",
      "       if session_data.get('utm_parameters'):\n",
      "           return classify_by_utm(session_data.utm_parameters)\n",
      "       return 'unknown'\n",
      "   ```\n",
      "\n",
      "2. Unknown Referrers\n",
      "   ```python\n",
      "   def classify_unknown_referrer(referrer_domain):\n",
      "       if is_ip_address(referrer_domain):\n",
      "           return 'internal'\n",
      "       if matches_spam_patterns(referrer_domain):\n",
      "           return 'filtered'\n",
      "       return 'other_referral'\n",
      "   ```\n",
      "\n",
      "3. Invalid UTM Parameters\n",
      "   ```python\n",
      "   def validate_utm(utm_params):\n",
      "       if not utm_params.get('source'):\n",
      "           return 'invalid_utm'\n",
      "       if utm_params.get('medium') not in VALID_MEDIUMS:\n",
      "           utm_params['medium'] = 'other'\n",
      "       return utm_params\n",
      "   ```\n",
      "\n",
      "IMPLEMENTATION SEQUENCE:\n",
      "\n",
      "1. First Pass: Basic Classification\n",
      "   ```python\n",
      "   def basic_classification(visit):\n",
      "       if visit.has_utm():\n",
      "           return classify_by_utm(visit)\n",
      "       if visit.has_referrer():\n",
      "           return classify_by_referrer(visit)\n",
      "       return 'direct'\n",
      "   ```\n",
      "\n",
      "2. Second Pass: Enrichment\n",
      "   ```python\n",
      "   def enrich_classification(classification, visit):\n",
      "       if classification.is_search():\n",
      "           add_search_metadata(classification, visit)\n",
      "       if classification.is_social():\n",
      "           add_social_metadata(classification, visit)\n",
      "       return classification\n",
      "   ```\n",
      "\n",
      "3. Final Pass: Validation\n",
      "   ```python\n",
      "   def validate_classification(classification):\n",
      "       assert classification.category in VALID_CATEGORIES\n",
      "       assert classification.source is not None\n",
      "       assert classification.medium is not None\n",
      "       return classification\n",
      "   ```\n",
      "\n",
      "DATA STRUCTURE OUTPUT:\n",
      "```python\n",
      "TrafficSource = {\n",
      "    'category': str,  # Primary classification\n",
      "    'source': str,    # Specific source\n",
      "    'medium': str,    # Traffic medium\n",
      "    'campaign': str,  # Campaign name if available\n",
      "    'metadata': {     # Additional context\n",
      "        'search_engine': str,\n",
      "        'search_query': str,\n",
      "        'social_platform': str,\n",
      "        'referrer_domain': str\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "This breakdown makes the classification process more manageable and testable. Each sub-task has clear inputs and outputs, and edge cases are explicitly handled. Would you like me to elaborate on any particular aspect?\n"
     ]
    }
   ],
   "source": [
    "message_history = nodes_to_message_history(nodes_validate)\n",
    "\n",
    "prompt_decompose = \"\"\"The 'Traffic Sources Classification' step is complex.\n",
    "Decompose it into smaller sub-tasks:\n",
    "\n",
    "1. List each sub-task needed\n",
    "2. Explain the logic for each classification category\n",
    "3. Describe how to handle edge cases (unknown referrers, missing data)\n",
    "\n",
    "Keep the sub-tasks atomic and implementable.\"\"\"\n",
    "\n",
    "agent_run_decompose, nodes_decompose = await run_agent(\n",
    "    agent_planner, prompt_decompose, message_history=message_history\n",
    ")\n",
    "\n",
    "print(\"Decomposed Sub-Tasks:\")\n",
    "print(agent_run_decompose.result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step4",
   "metadata": {},
   "source": [
    "## Step 4: Implement From the Plan\n",
    "\n",
    "Now implement the solution following the validated, decomposed plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "implement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation (from plan):\n",
      "Here's an implementation of the traffic sources classification pipeline:\n",
      "\n",
      "```python\n",
      "from dataclasses import dataclass\n",
      "from typing import Dict, Optional, List\n",
      "import pandas as pd\n",
      "import boto3\n",
      "from urllib.parse import urlparse, parse_qs\n",
      "import logging\n",
      "from datetime import datetime\n",
      "\n",
      "# Type definitions\n",
      "@dataclass\n",
      "class UTMParameters:\n",
      "    source: Optional[str]\n",
      "    medium: Optional[str]\n",
      "    campaign: Optional[str]\n",
      "\n",
      "@dataclass\n",
      "class TrafficSource:\n",
      "    category: str\n",
      "    source: str\n",
      "    medium: str\n",
      "    campaign: Optional[str]\n",
      "    metadata: Dict\n",
      "\n",
      "# Constants\n",
      "SEARCH_ENGINES = {\n",
      "    'google.com': {'param': 'q', 'name': 'Google'},\n",
      "    'bing.com': {'param': 'q', 'name': 'Bing'},\n",
      "    'yahoo.com': {'param': 'p', 'name': 'Yahoo'}\n",
      "}\n",
      "\n",
      "SOCIAL_PLATFORMS = {\n",
      "    'facebook.com': 'Facebook',\n",
      "    'twitter.com': 'Twitter',\n",
      "    't.co': 'Twitter',\n",
      "    'linkedin.com': 'LinkedIn',\n",
      "    'instagram.com': 'Instagram'\n",
      "}\n",
      "\n",
      "class TrafficSourceClassifier:\n",
      "    def __init__(self):\n",
      "        self.s3_client = boto3.client('s3')\n",
      "        self.logger = logging.getLogger(__name__)\n",
      "\n",
      "    def parse_referrer_url(self, url: Optional[str]) -> Optional[str]:\n",
      "        \"\"\"\n",
      "        Parse and normalize referrer URL.\n",
      "        \n",
      "        Args:\n",
      "            url: Raw referrer URL\n",
      "            \n",
      "        Returns:\n",
      "            Normalized domain or None\n",
      "        \"\"\"\n",
      "        if not url:\n",
      "            return None\n",
      "        \n",
      "        try:\n",
      "            parsed = urlparse(url)\n",
      "            domain = parsed.netloc.lower()\n",
      "            return domain.replace('www.', '')\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error parsing URL {url}: {str(e)}\")\n",
      "            return None\n",
      "\n",
      "    def extract_utm_parameters(self, url: str) -> UTMParameters:\n",
      "        \"\"\"\n",
      "        Extract UTM parameters from URL.\n",
      "        \n",
      "        Args:\n",
      "            url: URL containing UTM parameters\n",
      "            \n",
      "        Returns:\n",
      "            UTMParameters object\n",
      "        \"\"\"\n",
      "        try:\n",
      "            parsed = urlparse(url)\n",
      "            params = parse_qs(parsed.query)\n",
      "            \n",
      "            return UTMParameters(\n",
      "                source=params.get('utm_source', [None])[0],\n",
      "                medium=params.get('utm_medium', [None])[0],\n",
      "                campaign=params.get('utm_campaign', [None])[0]\n",
      "            )\n",
      "        except Exception as e:\n",
      "            self.logger.error(f\"Error extracting UTM parameters: {str(e)}\")\n",
      "            return UTMParameters(None, None, None)\n",
      "\n",
      "    def detect_search_engine(self, domain: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Detect if domain is a search engine.\n",
      "        \n",
      "        Args:\n",
      "            domain: Referrer domain\n",
      "            \n",
      "        Returns:\n",
      "            Dict with search engine info\n",
      "        \"\"\"\n",
      "        if domain in SEARCH_ENGINES:\n",
      "            return {\n",
      "                'is_search': True,\n",
      "                'engine': SEARCH_ENGINES[domain]['name']\n",
      "            }\n",
      "        return {'is_search': False, 'engine': None}\n",
      "\n",
      "    def classify_social_media(self, domain: str) -> Dict:\n",
      "        \"\"\"\n",
      "        Classify social media traffic.\n",
      "        \n",
      "        Args:\n",
      "            domain: Referrer domain\n",
      "            \n",
      "        Returns:\n",
      "            Dict with social classification\n",
      "        \"\"\"\n",
      "        if domain in SOCIAL_PLATFORMS:\n",
      "            return {\n",
      "                'is_social': True,\n",
      "                'platform': SOCIAL_PLATFORMS[domain]\n",
      "            }\n",
      "        return {'is_social': False, 'platform': None}\n",
      "\n",
      "    def categorize_traffic(self, visit_data: Dict) -> TrafficSource:\n",
      "        \"\"\"\n",
      "        Categorize traffic based on referrer and UTM data.\n",
      "        \n",
      "        Args:\n",
      "            visit_data: Dict containing visit information\n",
      "            \n",
      "        Returns:\n",
      "            TrafficSource classification\n",
      "        \"\"\"\n",
      "        referrer = visit_data.get('referrer')\n",
      "        utm = visit_data.get('utm_parameters')\n",
      "        \n",
      "        # Direct traffic\n",
      "        if not referrer and not utm:\n",
      "            return TrafficSource(\n",
      "                category='direct',\n",
      "                source='direct',\n",
      "                medium='none',\n",
      "                campaign=None,\n",
      "                metadata={}\n",
      "            )\n",
      "        \n",
      "        # UTM parameters present\n",
      "        if utm and utm.source:\n",
      "            return TrafficSource(\n",
      "                category='campaign',\n",
      "                source=utm.source,\n",
      "                medium=utm.medium or 'other',\n",
      "                campaign=utm.campaign,\n",
      "                metadata={'utm_present': True}\n",
      "            )\n",
      "        \n",
      "        # Referrer-based classification\n",
      "        domain = self.parse_referrer_url(referrer)\n",
      "        if not domain:\n",
      "            return TrafficSource(\n",
      "                category='unknown',\n",
      "                source='unknown',\n",
      "                medium='unknown',\n",
      "                campaign=None,\n",
      "                metadata={}\n",
      "            )\n",
      "        \n",
      "        search_info = self.detect_search_engine(domain)\n",
      "        if search_info['is_search']:\n",
      "            return TrafficSource(\n",
      "                category='organic_search',\n",
      "                source=search_info['engine'],\n",
      "                medium='organic',\n",
      "                campaign=None,\n",
      "                metadata={'search_engine': search_info['engine']}\n",
      "            )\n",
      "        \n",
      "        social_info = self.classify_social_media(domain)\n",
      "        if social_info['is_social']:\n",
      "            return TrafficSource(\n",
      "                category='social',\n",
      "                source=social_info['platform'],\n",
      "                medium='social',\n",
      "                campaign=None,\n",
      "                metadata={'social_platform': social_info['platform']}\n",
      "            )\n",
      "        \n",
      "        return TrafficSource(\n",
      "            category='referral',\n",
      "            source=domain,\n",
      "            medium='referral',\n",
      "            campaign=None,\n",
      "            metadata={'referrer_domain': domain}\n",
      "        )\n",
      "\n",
      "    def process_batch(self, df: pd.DataFrame) -> pd.DataFrame:\n",
      "        \"\"\"\n",
      "        Process a batch of traffic data.\n",
      "        \n",
      "        Args:\n",
      "            df: DataFrame with raw traffic data\n",
      "            \n",
      "        Returns:\n",
      "            DataFrame with classified traffic sources\n",
      "        \"\"\"\n",
      "        results = []\n",
      "        \n",
      "        for _, row in df.iterrows():\n",
      "            visit_data = {\n",
      "                'referrer': row.get('referrer'),\n",
      "                'utm_parameters': self.extract_utm_parameters(row.get('page_url', ''))\n",
      "            }\n",
      "            \n",
      "            classification = self.categorize_traffic(visit_data)\n",
      "            results.append({\n",
      "                'session_id': row.get('session_id'),\n",
      "                'timestamp': row.get('timestamp'),\n",
      "                'traffic_category': classification.category,\n",
      "                'traffic_source': classification.source,\n",
      "                'traffic_medium': classification.medium,\n",
      "                'campaign': classification.campaign,\n",
      "                **classification.metadata\n",
      "            })\n",
      "            \n",
      "        return pd.DataFrame(results)\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main pipeline orchestration function.\"\"\"\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "    \n",
      "    # Initialize classifier\n",
      "    classifier = TrafficSourceClassifier()\n",
      "    \n",
      "    try:\n",
      "        # Configuration\n",
      "        input_bucket = 'your-input-bucket'\n",
      "        output_bucket = 'your-output-bucket'\n",
      "        date = datetime.now().strftime('%Y-%m-%d')\n",
      "        \n",
      "        # Read input data\n",
      "        s3 = boto3.client('s3')\n",
      "        input_key = f'traffic_data/{date}/raw_traffic.csv'\n",
      "        \n",
      "        logger.info(f\"Reading input data from s3://{input_bucket}/{input_key}\")\n",
      "        obj = s3.get_object(Bucket=input_bucket, Key=input_key)\n",
      "        df = pd.read_csv(obj['Body'])\n",
      "        \n",
      "        # Process data\n",
      "        logger.info(\"Processing traffic data\")\n",
      "        results_df = classifier.process_batch(df)\n",
      "        \n",
      "        # Write results\n",
      "        output_key = f'traffic_data/{date}/classified_traffic.csv'\n",
      "        logger.info(f\"Writing results to s3://{output_bucket}/{output_key}\")\n",
      "        \n",
      "        csv_buffer = results_df.to_csv(index=False)\n",
      "        s3.put_object(\n",
      "            Bucket=output_bucket,\n",
      "            Key=output_key,\n",
      "            Body=csv_buffer\n",
      "        )\n",
      "        \n",
      "        logger.info(\"Pipeline completed successfully\")\n",
      "        \n",
      "    except Exception as e:\n",
      "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "This implementation includes:\n",
      "\n",
      "1. Type definitions using dataclasses for structured data\n",
      "2. Constants for search engines and social platforms\n",
      "3. A main TrafficSourceClassifier class with methods for:\n",
      "   - URL parsing\n",
      "   - UTM parameter extraction\n",
      "   - Search engine detection\n",
      "   - Social media classification\n",
      "   - Traffic categorization\n",
      "4. Batch processing capability using pandas\n",
      "5. S3 integration for input/output\n",
      "6. Error handling and logging\n",
      "7. A main() function that orchestrates the pipeline\n",
      "\n",
      "To use this:\n",
      "\n",
      "1. Configure AWS credentials\n",
      "2. Set your S3 bucket names in main()\n",
      "3. Ensure input data is in the expected format (CSV with columns: session_id, timestamp, referrer, page_url)\n",
      "4. Run the script\n",
      "\n",
      "The output will be a CSV file with classified traffic sources, including:\n",
      "- Traffic category\n",
      "- Source\n",
      "- Medium\n",
      "- Campaign (if present)\n",
      "- Additional metadata based on the classification\n",
      "\n",
      "You can extend this by:\n",
      "\n",
      "1. Adding more traffic source categories\n",
      "2. Enhancing the classification logic\n",
      "3. Adding validation steps\n",
      "4. Implementing parallel processing for large datasets\n",
      "5. Adding monitoring and alerts\n",
      "\n",
      "Would you like me to elaborate on any part of the implementation?\n"
     ]
    }
   ],
   "source": [
    "system_prompt_implementer = \"\"\"You are a Python developer. Implement code following the given plan.\n",
    "Write clean, well-structured code. Follow the plan exactly.\"\"\"\n",
    "\n",
    "message_history = nodes_to_message_history(nodes_decompose)\n",
    "\n",
    "prompt_implement = \"\"\"Implement the data pipeline following the plan.\n",
    "\n",
    "Create Python code that:\n",
    "1. Follows the step structure from the plan\n",
    "2. Implements each step as a separate function\n",
    "3. Includes type hints and docstrings\n",
    "4. Has a main() function that orchestrates the pipeline\n",
    "\n",
    "Use boto3 for S3, pandas for data processing.\"\"\"\n",
    "\n",
    "agent_implementer = get_agent(\n",
    "    config_name=\"fast\", system_prompt=system_prompt_implementer\n",
    ")\n",
    "agent_run_impl, _ = await run_agent(\n",
    "    agent_implementer, prompt_implement, message_history=message_history\n",
    ")\n",
    "\n",
    "print(\"Implementation (from plan):\")\n",
    "print(agent_run_impl.result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927cc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_agentic_patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
