{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Tree of Thought Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_patterns.core.agents import get_agent, run_agent\n",
    "from agentic_patterns.core.agents.utils import nodes_to_message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_problem",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Design a file deduplication system for a cloud storage service.\n",
    "\n",
    "This problem benefits from Tree of Thought because:\n",
    "- Multiple valid algorithms exist (hashing strategies, chunking approaches)\n",
    "- Each has different trade-offs (accuracy vs speed vs storage overhead)\n",
    "- Requirements vary based on use case (exact duplicates vs similar files)\n",
    "- Exploring alternatives prevents committing to an approach with deal-breaker limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "problem_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"Design a file deduplication system for a cloud storage service.\n",
    "\n",
    "Requirements:\n",
    "- Handle millions of files (documents, images, videos)\n",
    "- Detect duplicate files to save storage costs\n",
    "- Fast upload processing (users shouldn't wait long)\n",
    "- Minimize false positives (marking different files as duplicates)\n",
    "- Handle files from 1KB to 5GB\n",
    "\n",
    "Propose different deduplication approaches.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step1",
   "metadata": {},
   "source": [
    "## Step 1: Generate Multiple Deduplication Approaches\n",
    "\n",
    "Instead of committing to one algorithm, generate 3 different approaches to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generate_approaches",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Approaches:\n",
      "# File Deduplication System Approaches\n",
      "\n",
      "## Approach A: Content Hash-Based Deduplication\n",
      "**Core Algorithm:** Cryptographic hashing (SHA-256) of entire file content\n",
      "\n",
      "Calculates a hash of the complete file content and stores it as a unique identifier. When a new file is uploaded, its hash is computed and checked against existing hashes in the database. If a match is found, only a reference to the existing file is stored rather than duplicating the content.\n",
      "\n",
      "## Approach B: Chunked Deduplication with Rolling Hash\n",
      "**Core Algorithm:** Rabin fingerprinting with variable-size chunking\n",
      "\n",
      "Splits files into variable-sized chunks (4KB-64KB) using content-defined boundaries based on rolling hash windows. Each chunk is hashed separately, allowing detection of partial file duplicates and efficient handling of modified files. Only unique chunks are stored, with files reconstructed from chunk references.\n",
      "\n",
      "## Approach C: Fast Pre-filter with Perceptual Hashing\n",
      "**Core Algorithm:** Two-tier system using file metadata + perceptual hashing for media\n",
      "\n",
      "First filters candidates using size and quick checksums (first/last 1MB), then applies perceptual hashing (pHash) for images/videos to detect near-duplicates. For documents, falls back to full content hashing only for size-matched candidates. This minimizes expensive hash operations on obviously different files.\n"
     ]
    }
   ],
   "source": [
    "agent = get_agent()\n",
    "\n",
    "prompt_generate = f\"\"\"{problem}\n",
    "\n",
    "Generate exactly 3 different deduplication approaches. For each approach, provide:\n",
    "- A name (Approach A, B, C)\n",
    "- The core algorithm/technique used\n",
    "- Brief description (2-3 sentences) of how it works\n",
    "\n",
    "Keep each description concise. Focus on the fundamental differences between approaches.\"\"\"\n",
    "\n",
    "agent_run_1, nodes_1 = await run_agent(agent, prompt_generate)\n",
    "\n",
    "assert agent_run_1 is not None and agent_run_1.result is not None\n",
    "print(\"Generated Approaches:\")\n",
    "print(agent_run_1.result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step2",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate Each Approach\n",
    "\n",
    "Score each approach on key system requirements.\n",
    "This evaluation helps identify which paths are worth exploring further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "evaluate_approaches",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n",
      "# Evaluation of Deduplication Approaches\n",
      "\n",
      "## Approach A: Content Hash-Based Deduplication\n",
      "\n",
      "| Criterion | Rating | Justification |\n",
      "|-----------|--------|---------------|\n",
      "| **Accuracy** | 5/5 | SHA-256 has virtually zero collision probability; perfect detection of exact duplicates with no false positives |\n",
      "| **Performance** | 3/5 | Must hash entire file (slow for 5GB files); simple database lookup is fast, but upload processing is bottlenecked by hashing |\n",
      "| **Storage Overhead** | 5/5 | Minimal - only 32 bytes per file hash plus reference count; very efficient |\n",
      "| **Scalability** | 4/5 | Database lookups scale well with indexing; hash computation is parallelizable, but large files still take time |\n",
      "\n",
      "**Total Score: 17/20**\n",
      "\n",
      "Simple and reliable, but slower for large files since entire content must be processed before deduplication decision.\n",
      "\n",
      "---\n",
      "\n",
      "## Approach B: Chunked Deduplication with Rolling Hash\n",
      "\n",
      "| Criterion | Rating | Justification |\n",
      "|-----------|--------|---------------|\n",
      "| **Accuracy** | 5/5 | Detects both exact and partial duplicates; cryptographic chunk hashes eliminate false positives |\n",
      "| **Performance** | 4/5 | Can start storing chunks immediately during upload; parallel chunk processing; slight overhead from chunking algorithm |\n",
      "| **Storage Overhead** | 2/5 | Significant - stores hash for every chunk (millions of chunks for large files); requires chunk-to-file mapping tables |\n",
      "| **Scalability** | 3/5 | Metadata grows enormously (chunk index); better deduplication ratio but more complex database queries and management |\n",
      "\n",
      "**Total Score: 14/20**\n",
      "\n",
      "Best deduplication efficiency and handles file modifications well, but high metadata overhead becomes problematic at scale.\n",
      "\n",
      "---\n",
      "\n",
      "## Approach C: Fast Pre-filter with Perceptual Hashing\n",
      "\n",
      "| Criterion | Rating | Justification |\n",
      "|-----------|--------|---------------|\n",
      "| **Accuracy** | 3/5 | Perceptual hashing allows false positives on similar media; metadata pre-filter is accurate but misses some duplicates with different sizes |\n",
      "| **Performance** | 5/5 | Fast filtering eliminates 90%+ of candidates immediately; full hashing only on small subset; perceptual hashing is very fast |\n",
      "| **Storage Overhead** | 4/5 | Moderate - file size, partial checksums, perceptual hashes; more than Approach A but manageable |\n",
      "| **Scalability** | 5/5 | Multi-tier filtering keeps database queries efficient; different strategies per file type optimize resource usage |\n",
      "\n",
      "**Total Score: 17/20**\n",
      "\n",
      "Excellent performance through smart filtering, though accuracy tradeoffs exist for near-duplicate media detection.\n",
      "\n",
      "---\n",
      "\n",
      "# Final Ranking\n",
      "\n",
      "## 1st Place (Tie): Approach A & Approach C - 17/20\n",
      "\n",
      "**Approach A** for scenarios prioritizing **accuracy and simplicity**\n",
      "- Best choice when exact duplicate detection is critical\n",
      "- Suitable for document-heavy workloads\n",
      "- Easier to implement and maintain\n",
      "\n",
      "**Approach C** for scenarios prioritizing **user experience and mixed content**\n",
      "- Best choice when upload speed is critical\n",
      "- Ideal for consumer cloud storage with photos/videos\n",
      "- Good balance of all factors\n",
      "\n",
      "## 2nd Place: Approach B - 14/20\n",
      "\n",
      "**Best for specialized use cases:**\n",
      "- Backup systems where incremental deduplication matters\n",
      "- Enterprise storage with frequently modified files\n",
      "- When storage savings justify metadata complexity\n",
      "\n",
      "**Recommendation:** Start with **Approach C** for most cloud storage services, as it provides the best user experience. Consider **Approach A** if simplicity and perfect accuracy are more important than upload speed.\n"
     ]
    }
   ],
   "source": [
    "message_history = nodes_to_message_history(nodes_1)\n",
    "\n",
    "prompt_evaluate = \"\"\"Evaluate each of the 3 approaches you generated.\n",
    "\n",
    "For each approach, rate it on these criteria (1-5 scale, 5 is best):\n",
    "1. Accuracy (avoiding false positives/negatives)\n",
    "2. Performance (speed of duplicate detection)\n",
    "3. Storage overhead (metadata storage requirements)\n",
    "4. Scalability (handles millions of files)\n",
    "\n",
    "Provide a total score and brief justification for each rating.\n",
    "Then rank the approaches from best to worst.\"\"\"\n",
    "\n",
    "agent_run_2, nodes_2 = await run_agent(\n",
    "    agent, prompt_evaluate, message_history=message_history\n",
    ")\n",
    "\n",
    "assert agent_run_2 is not None and agent_run_2.result is not None\n",
    "print(\"\\nEvaluation:\")\n",
    "print(agent_run_2.result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step3",
   "metadata": {},
   "source": [
    "## Step 3: Expand the Top 2 Approaches\n",
    "\n",
    "Rather than continuing all branches, prune the lowest-scoring approach.\n",
    "Expand the top 2 by developing detailed implementation designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expand_top",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expanded Implementations:\n",
      "# Detailed Implementation Design for Top 2 Approaches\n",
      "\n",
      "## Approach A: Content Hash-Based Deduplication\n",
      "\n",
      "### 1. Data Structures\n",
      "\n",
      "#### Database Tables\n",
      "\n",
      "```sql\n",
      "-- Main file metadata table\n",
      "CREATE TABLE files (\n",
      "    file_id UUID PRIMARY KEY,\n",
      "    user_id UUID NOT NULL,\n",
      "    filename VARCHAR(512) NOT NULL,\n",
      "    content_hash CHAR(64) NOT NULL,  -- SHA-256 hex\n",
      "    file_size BIGINT NOT NULL,\n",
      "    mime_type VARCHAR(100),\n",
      "    uploaded_at TIMESTAMP NOT NULL,\n",
      "    INDEX idx_user_files (user_id, uploaded_at),\n",
      "    INDEX idx_content_hash (content_hash),\n",
      "    FOREIGN KEY (content_hash) REFERENCES file_contents(content_hash)\n",
      ");\n",
      "\n",
      "-- Deduplicated content storage references\n",
      "CREATE TABLE file_contents (\n",
      "    content_hash CHAR(64) PRIMARY KEY,  -- SHA-256 hex\n",
      "    storage_path VARCHAR(1024) NOT NULL,  -- S3 key or filesystem path\n",
      "    compressed_size BIGINT NOT NULL,\n",
      "    reference_count INT NOT NULL DEFAULT 1,\n",
      "    first_uploaded_at TIMESTAMP NOT NULL,\n",
      "    last_accessed_at TIMESTAMP,\n",
      "    INDEX idx_ref_count (reference_count)\n",
      ");\n",
      "\n",
      "-- User quotas tracking\n",
      "CREATE TABLE user_storage_quotas (\n",
      "    user_id UUID PRIMARY KEY,\n",
      "    total_files_size BIGINT NOT NULL DEFAULT 0,\n",
      "    unique_bytes_stored BIGINT NOT NULL DEFAULT 0,\n",
      "    file_count INT NOT NULL DEFAULT 0,\n",
      "    updated_at TIMESTAMP NOT NULL\n",
      ");\n",
      "```\n",
      "\n",
      "#### In-Memory Cache (Redis)\n",
      "\n",
      "```\n",
      "Key Pattern: hash:checking:{partial_hash}\n",
      "Value: {status: \"processing\"|\"complete\", workers: [worker_ids]}\n",
      "TTL: 300 seconds\n",
      "\n",
      "Key Pattern: hash:result:{content_hash}\n",
      "Value: {storage_path: \"s3://...\", size: 12345}\n",
      "TTL: 3600 seconds\n",
      "```\n",
      "\n",
      "### 2. Upload Flow\n",
      "\n",
      "```\n",
      "Step 1: Client Preparation\n",
      "┌─────────────────────────────────────┐\n",
      "│ Client calculates SHA-256 locally   │\n",
      "│ (optional, for fast rejection)      │\n",
      "│ Sends: filename, size, hash         │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "\n",
      "Step 2: Initial Duplicate Check\n",
      "┌─────────────────────────────────────┐\n",
      "│ Query: SELECT storage_path          │\n",
      "│        FROM file_contents           │\n",
      "│        WHERE content_hash = ?       │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "        ┌────────┴────────┐\n",
      "        │                 │\n",
      "    Found           Not Found\n",
      "        │                 │\n",
      "        ↓                 ↓\n",
      "┌───────────────┐  ┌──────────────────┐\n",
      "│ Skip Upload   │  │ Proceed Upload   │\n",
      "│ (Step 6)      │  │ (Step 3)         │\n",
      "└───────────────┘  └──────────────────┘\n",
      "\n",
      "Step 3: Generate Upload URL\n",
      "┌─────────────────────────────────────┐\n",
      "│ Generate presigned S3 URL           │\n",
      "│ Temp path: uploads/{uuid}/{filename}│\n",
      "│ Return upload URL to client         │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "\n",
      "Step 4: Client Upload\n",
      "┌─────────────────────────────────────┐\n",
      "│ Client streams file to S3           │\n",
      "│ Multipart upload for files > 100MB │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "\n",
      "Step 5: Server-Side Hash Verification\n",
      "┌─────────────────────────────────────┐\n",
      "│ Trigger Lambda/Worker on S3 event  │\n",
      "│ Stream file from S3, compute SHA256│\n",
      "│ Compare with client-provided hash  │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "        ┌────────┴────────┐\n",
      "        │                 │\n",
      "    Match            Mismatch\n",
      "        │                 │\n",
      "        ↓                 ↓\n",
      "┌───────────────┐  ┌──────────────────┐\n",
      "│ Continue      │  │ Delete upload    │\n",
      "│ (Step 6)      │  │ Return error     │\n",
      "└───────────────┘  └──────────────────┘\n",
      "\n",
      "Step 6: Atomic Deduplication Transaction\n",
      "┌─────────────────────────────────────┐\n",
      "│ BEGIN TRANSACTION                   │\n",
      "│                                     │\n",
      "│ 1. Check file_contents again:      │\n",
      "│    SELECT * FROM file_contents      │\n",
      "│    WHERE content_hash = ? FOR UPDATE│\n",
      "│                                     │\n",
      "│ IF EXISTS:                          │\n",
      "│   - Increment reference_count       │\n",
      "│   - Delete uploaded temp file       │\n",
      "│   - Use existing storage_path       │\n",
      "│ ELSE:                               │\n",
      "│   - Move temp file to permanent:    │\n",
      "│     content/{hash[0:2]}/{hash}      │\n",
      "│   - INSERT INTO file_contents       │\n",
      "│                                     │\n",
      "│ 2. INSERT INTO files                │\n",
      "│                                     │\n",
      "│ 3. UPDATE user_storage_quotas       │\n",
      "│    - Add file_size to total         │\n",
      "│    - Add to unique_bytes if new     │\n",
      "│                                     │\n",
      "│ COMMIT                              │\n",
      "└─────────────────────────────────────┘\n",
      "```\n",
      "\n",
      "### 3. Query Flow (Duplicate Check)\n",
      "\n",
      "```python\n",
      "def check_duplicate(user_id, filename, content_hash):\n",
      "    \"\"\"\n",
      "    Check if file is duplicate for this user\n",
      "    \"\"\"\n",
      "    # Step 1: Check if user already has this exact file\n",
      "    existing_file = db.query(\"\"\"\n",
      "        SELECT file_id, filename, file_size, uploaded_at\n",
      "        FROM files\n",
      "        WHERE user_id = ? AND content_hash = ?\n",
      "        LIMIT 1\n",
      "    \"\"\", user_id, content_hash)\n",
      "    \n",
      "    if existing_file:\n",
      "        return {\n",
      "            'is_duplicate': True,\n",
      "            'duplicate_type': 'user_owns',\n",
      "            'existing_file': existing_file,\n",
      "            'can_skip_upload': True\n",
      "        }\n",
      "    \n",
      "    # Step 2: Check if content exists globally (owned by other users)\n",
      "    content_exists = db.query(\"\"\"\n",
      "        SELECT storage_path, compressed_size\n",
      "        FROM file_contents\n",
      "        WHERE content_hash = ?\n",
      "    \"\"\", content_hash)\n",
      "    \n",
      "    if content_exists:\n",
      "        return {\n",
      "            'is_duplicate': False,\n",
      "            'content_exists': True,\n",
      "            'can_skip_upload': True,  # Can link to existing content\n",
      "            'storage_path': content_exists.storage_path\n",
      "        }\n",
      "    \n",
      "    # Step 3: No duplicate found\n",
      "    return {\n",
      "        'is_duplicate': False,\n",
      "        'content_exists': False,\n",
      "        'can_skip_upload': False\n",
      "    }\n",
      "```\n",
      "\n",
      "### 4. Storage Requirements (1 Million Files)\n",
      "\n",
      "#### Database Storage\n",
      "\n",
      "```\n",
      "files table:\n",
      "- file_id: 16 bytes (UUID)\n",
      "- user_id: 16 bytes (UUID)\n",
      "- filename: ~100 bytes average\n",
      "- content_hash: 64 bytes\n",
      "- file_size: 8 bytes\n",
      "- mime_type: ~30 bytes average\n",
      "- uploaded_at: 8 bytes\n",
      "- Total per row: ~242 bytes\n",
      "- 1M files: ~242 MB\n",
      "\n",
      "file_contents table (assuming 20% deduplication):\n",
      "- content_hash: 64 bytes\n",
      "- storage_path: ~100 bytes average\n",
      "- compressed_size: 8 bytes\n",
      "- reference_count: 4 bytes\n",
      "- timestamps: 16 bytes\n",
      "- Total per row: ~192 bytes\n",
      "- 800K unique files: ~154 MB\n",
      "\n",
      "user_storage_quotas (assuming 10K users):\n",
      "- user_id: 16 bytes\n",
      "- counters: 24 bytes\n",
      "- updated_at: 8 bytes\n",
      "- Total: ~48 bytes × 10K = ~480 KB\n",
      "\n",
      "Indexes (estimated 2x data size): ~792 MB\n",
      "\n",
      "Total Database: ~1.2 GB\n",
      "```\n",
      "\n",
      "#### Cache Storage (Redis)\n",
      "\n",
      "```\n",
      "Active upload tracking:\n",
      "- Average 100 concurrent uploads\n",
      "- ~500 bytes per tracking entry\n",
      "- Total: ~50 KB\n",
      "\n",
      "Hash result cache (hot data):\n",
      "- Cache 100K most recent lookups\n",
      "- ~200 bytes per entry\n",
      "- Total: ~20 MB\n",
      "\n",
      "Total Cache: ~20 MB\n",
      "```\n",
      "\n",
      "#### Total Storage Overhead: **~1.22 GB for 1M files**\n",
      "\n",
      "---\n",
      "\n",
      "## Approach C: Fast Pre-filter with Perceptual Hashing\n",
      "\n",
      "### 1. Data Structures\n",
      "\n",
      "#### Database Tables\n",
      "\n",
      "```sql\n",
      "-- Main file metadata with pre-filter fields\n",
      "CREATE TABLE files (\n",
      "    file_id UUID PRIMARY KEY,\n",
      "    user_id UUID NOT NULL,\n",
      "    filename VARCHAR(512) NOT NULL,\n",
      "    file_size BIGINT NOT NULL,\n",
      "    content_hash CHAR(64) NOT NULL,  -- SHA-256, computed lazily\n",
      "    mime_type VARCHAR(100),\n",
      "    \n",
      "    -- Pre-filter fields\n",
      "    quick_hash CHAR(32),  -- MD5 of first/last 1MB\n",
      "    perceptual_hash BIGINT,  -- pHash for images/videos\n",
      "    \n",
      "    uploaded_at TIMESTAMP NOT NULL,\n",
      "    \n",
      "    INDEX idx_user_files (user_id, uploaded_at),\n",
      "    INDEX idx_quick_filter (file_size, quick_hash),\n",
      "    INDEX idx_content_hash (content_hash),\n",
      "    INDEX idx_perceptual (perceptual_hash)\n",
      ");\n",
      "\n",
      "-- Deduplicated content storage\n",
      "CREATE TABLE file_contents (\n",
      "    content_hash CHAR(64) PRIMARY KEY,\n",
      "    storage_path VARCHAR(1024) NOT NULL,\n",
      "    compressed_size BIGINT NOT NULL,\n",
      "    reference_count INT NOT NULL DEFAULT 1,\n",
      "    first_uploaded_at TIMESTAMP NOT NULL,\n",
      "    INDEX idx_ref_count (reference_count)\n",
      ");\n",
      "\n",
      "-- Media similarity clusters (for perceptual deduplication)\n",
      "CREATE TABLE media_clusters (\n",
      "    cluster_id UUID PRIMARY KEY,\n",
      "    representative_file_id UUID NOT NULL,\n",
      "    perceptual_hash_center BIGINT NOT NULL,\n",
      "    hamming_radius INT NOT NULL DEFAULT 5,\n",
      "    file_count INT NOT NULL DEFAULT 1,\n",
      "    created_at TIMESTAMP NOT NULL,\n",
      "    INDEX idx_phash_search (perceptual_hash_center)\n",
      ");\n",
      "\n",
      "CREATE TABLE media_cluster_members (\n",
      "    cluster_id UUID NOT NULL,\n",
      "    file_id UUID NOT NULL,\n",
      "    hamming_distance INT NOT NULL,\n",
      "    PRIMARY KEY (cluster_id, file_id),\n",
      "    FOREIGN KEY (cluster_id) REFERENCES media_clusters(cluster_id),\n",
      "    FOREIGN KEY (file_id) REFERENCES files(file_id)\n",
      ");\n",
      "```\n",
      "\n",
      "#### In-Memory Bloom Filter\n",
      "\n",
      "```python\n",
      "class DeduplicationBloomFilter:\n",
      "    \"\"\"\n",
      "    Probabilistic filter for quick rejection\n",
      "    Size: 10MB for 1M files, 0.1% false positive rate\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        self.size = 80_000_000  # bits\n",
      "        self.hash_functions = 7\n",
      "        self.filter = bitarray(self.size)\n",
      "    \n",
      "    def add(self, file_size: int, quick_hash: str):\n",
      "        \"\"\"Add file signature to bloom filter\"\"\"\n",
      "        key = f\"{file_size}:{quick_hash}\"\n",
      "        for seed in range(self.hash_functions):\n",
      "            idx = hash((key, seed)) % self.size\n",
      "            self.filter[idx] = 1\n",
      "    \n",
      "    def might_exist(self, file_size: int, quick_hash: str) -> bool:\n",
      "        \"\"\"Check if file might be duplicate\"\"\"\n",
      "        key = f\"{file_size}:{quick_hash}\"\n",
      "        for seed in range(self.hash_functions):\n",
      "            idx = hash((key, seed)) % self.size\n",
      "            if not self.filter[idx]:\n",
      "                return False\n",
      "        return True\n",
      "```\n",
      "\n",
      "#### Redis Cache\n",
      "\n",
      "```\n",
      "Key: quick:check:{size}:{quick_hash}\n",
      "Value: {exists: true, file_ids: [...]}\n",
      "TTL: 1800 seconds\n",
      "\n",
      "Key: phash:similar:{phash}\n",
      "Value: {cluster_id: \"uuid\", distance: 3}\n",
      "TTL: 3600 seconds\n",
      "```\n",
      "\n",
      "### 2. Upload Flow\n",
      "\n",
      "```\n",
      "Step 1: Client Pre-Processing\n",
      "┌─────────────────────────────────────┐\n",
      "│ Client reads file metadata          │\n",
      "│ - file_size                         │\n",
      "│ - mime_type                         │\n",
      "│                                     │\n",
      "│ Client computes quick_hash:         │\n",
      "│ - Read first 1MB of file            │\n",
      "│ - Read last 1MB of file             │\n",
      "│ - MD5(first_block + last_block)     │\n",
      "│                                     │\n",
      "│ For images/videos (optional):       │\n",
      "│ - Generate thumbnail                │\n",
      "│ - Compute perceptual hash           │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "\n",
      "Step 2: Multi-Tier Duplicate Check\n",
      "┌─────────────────────────────────────┐\n",
      "│ Tier 1: Bloom Filter Check          │\n",
      "│ if NOT bloom.might_exist(size, qh): │\n",
      "│     return NO_DUPLICATE             │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "┌─────────────────────────────────────┐\n",
      "│ Tier 2: Quick Hash + Size Check     │\n",
      "│ Check Redis cache first, then DB:   │\n",
      "│                                     │\n",
      "│ candidates = SELECT file_id, content_hash│\n",
      "│   FROM files                        │\n",
      "│   WHERE user_id = ?                 │\n",
      "│   AND file_size = ?                 │\n",
      "│   AND quick_hash = ?                │\n",
      "│                                     │\n",
      "│ if candidates.empty():              │\n",
      "│     return LIKELY_NO_DUPLICATE      │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "┌─────────────────────────────────────┐\n",
      "│ Tier 3: Content Hash Check          │\n",
      "│ (Only for promising candidates)     │\n",
      "│                                     │\n",
      "│ For each candidate:                 │\n",
      "│   if candidate.content_hash != null:│\n",
      "│       if matches_client_hash:       │\n",
      "│           return EXACT_DUPLICATE    │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "┌─────────────────────────────────────┐\n",
      "│ Tier 4: Perceptual Check (Media)    │\n",
      "│ (Only for images/videos)            │\n",
      "│                                     │\n",
      "│ similar = SELECT c.*, f.file_id     │\n",
      "│   FROM media_clusters c             │\n",
      "│   JOIN media_cluster_members m      │\n",
      "│   WHERE hamming_distance(           │\n",
      "│       c.perceptual_hash_center,     │\n",
      "│       ?) <= 5                       │\n",
      "│                                     │\n",
      "│ if similar exists:                  │\n",
      "│     return SIMILAR_MEDIA (prompt user)│\n",
      "└─────────────────────────────────────┘\n",
      "\n",
      "Step 3: Conditional Upload\n",
      "┌─────────────────────────────────────┐\n",
      "│ IF exact duplicate found:           │\n",
      "│   - Link to existing content        │\n",
      "│   - Skip upload entirely            │\n",
      "│                                     │\n",
      "│ IF similar media found:             │\n",
      "│   - Show user preview of similar    │\n",
      "│   - Let user decide: upload or skip │\n",
      "│                                     │\n",
      "│ IF no duplicate:                    │\n",
      "│   - Generate presigned upload URL   │\n",
      "│   - Continue to Step 4              │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "\n",
      "Step 4: Streaming Upload with Progressive Hashing\n",
      "┌─────────────────────────────────────┐\n",
      "│ Client uploads to S3 (multipart)    │\n",
      "│                                     │\n",
      "│ Server-side streaming processor:    │\n",
      "│ - Reads chunks as they arrive       │\n",
      "│ - Updates SHA-256 hash incrementally│\n",
      "│ - Can detect duplicates mid-upload  │\n",
      "│   if quick_hash matches and size    │\n",
      "│   matches partial content           │\n",
      "└─────────────────────────────────────┘\n",
      "                 ↓\n",
      "\n",
      "Step 5: Post-Upload Processing\n",
      "┌─────────────────────────────────────┐\n",
      "│ Finalize SHA-256 hash               │\n",
      "│                                     │\n",
      "│ For media files:                    │\n",
      "│ - Generate perceptual hash          │\n",
      "│ - Find or create cluster            │\n",
      "│ - Add to media_cluster_members      │\n",
      "│                                     │\n",
      "│ BEGIN TRANSACTION                   │\n",
      "│   Check file_contents (by hash)     │\n",
      "│   IF exists:                        │\n",
      "│     - Increment ref count           │\n",
      "│     - Delete duplicate upload       │\n",
      "│   ELSE:                             │\n",
      "│     - Insert into file_contents     │\n",
      "│   END IF                            │\n",
      "│                                     │\n",
      "│   INSERT INTO files (all metadata)  │\n",
      "│                                     │\n",
      "│   Update bloom filter               │\n",
      "│   Update Redis cache                │\n",
      "│ COMMIT                              │\n",
      "└─────────────────────────────────────┘\n",
      "```\n",
      "\n",
      "### 3. Query Flow (Duplicate Check)\n",
      "\n",
      "```python\n",
      "def check_duplicate_multitiered(user_id, filename, file_size, \n",
      "                                quick_hash, mime_type, perceptual_hash=None):\n",
      "    \"\"\"\n",
      "    Multi-tier duplicate detection\n",
      "    \"\"\"\n",
      "    \n",
      "    # Tier 1: Bloom Filter (in-memory, microseconds)\n",
      "    if not bloom_filter.might_exist(file_size, quick_hash):\n",
      "        return {\n",
      "            'is_duplicate': False,\n",
      "            'confidence': 'high',\n",
      "            'check_level': 'bloom_filter'\n",
      "        }\n",
      "    \n",
      "    # Tier 2: Redis Cache Check (milliseconds)\n",
      "    cache_key = f\"quick:check:{file_size}:{quick_hash}\"\n",
      "    cached_result = redis.get(cache_key)\n",
      "    \n",
      "    if cached_result:\n",
      "        candidates = json.loads(cached_result)\n",
      "    else:\n",
      "        # Tier 3: Database Quick Filter\n",
      "        candidates = db.query(\"\"\"\n",
      "            SELECT file_id, filename, content_hash, perceptual_hash\n",
      "            FROM files\n",
      "            WHERE user_id = ?\n",
      "              AND file_size = ?\n",
      "              AND quick_hash = ?\n",
      "            LIMIT 10\n",
      "        \"\"\", user_id, file_size, quick_hash)\n",
      "        \n",
      "        # Cache for 30 minutes\n",
      "        redis.setex(cache_key, 1800, json.dumps(candidates))\n",
      "    \n",
      "    if not candidates:\n",
      "        return {\n",
      "            'is_duplicate': False,\n",
      "            'confidence': 'high',\n",
      "            'check_level': 'quick_hash'\n",
      "        }\n",
      "    \n",
      "    # Tier 4: Exact Hash Match\n",
      "    for candidate in candidates:\n",
      "        if candidate.content_hash:  # Already computed\n",
      "            # Would need full hash from client for comparison\n",
      "            return {\n",
      "                'is_duplicate': True,\n",
      "                'confidence': 'absolute',\n",
      "                'duplicate_of': candidate.file_id,\n",
      "                'check_level': 'content_hash'\n",
      "            }\n",
      "    \n",
      "    # Tier 5: Perceptual Hash (for media)\n",
      "    if perceptual_hash and mime_type.startswith(('image/', 'video/')):\n",
      "        similar_media = db.query(\"\"\"\n",
      "            SELECT m.file_id, f.filename, \n",
      "                   BIT_COUNT(c.perceptual_hash_center ^ ?) as distance\n",
      "            FROM media_clusters c\n",
      "            JOIN media_cluster_members m ON c.cluster_id = m.cluster_id\n",
      "            JOIN files f ON m.file_id = f.file_id\n",
      "            WHERE f.user_id = ?\n",
      "              AND BIT_COUNT(c.perceptual_hash_center ^ ?) <= 5\n",
      "            ORDER BY distance ASC\n",
      "            LIMIT 5\n",
      "        \"\"\", perceptual_hash, user_id, perceptual_hash)\n",
      "        \n",
      "        if similar_media:\n",
      "            return {\n",
      "                'is_duplicate': False,\n",
      "                'has_similar': True,\n",
      "                'confidence': 'medium',\n",
      "                'similar_files': similar_media,\n",
      "                'check_level': 'perceptual_hash',\n",
      "                'action_required': 'user_confirmation'\n",
      "            }\n",
      "    \n",
      "    # Likely unique, but needs full hash verification\n",
      "    return {\n",
      "        'is_duplicate': False,\n",
      "        'confidence': 'medium',\n",
      "        'check_level': 'quick_hash_no_match',\n",
      "        'requires_full_upload': True\n",
      "    }\n",
      "```\n",
      "\n",
      "### 4. Storage Requirements (1 Million Files)\n",
      "\n",
      "#### Database Storage\n",
      "\n",
      "```\n",
      "files table (with additional fields):\n",
      "- Basic fields: ~242 bytes (from Approach A)\n",
      "- quick_hash: 32 bytes\n",
      "- perceptual_hash: 8 bytes\n",
      "- Total per row: ~282 bytes\n",
      "- 1M files: ~282 MB\n",
      "\n",
      "file_contents table (20% deduplication):\n",
      "- Same as Approach A: ~154 MB\n",
      "\n",
      "media_clusters (assuming 30% of files are media, 10% cluster rate):\n",
      "- cluster_id: 16 bytes\n",
      "- representative_file_id: 16 bytes\n",
      "- perceptual_hash_center: 8 bytes\n",
      "- hamming_radius: 4 bytes\n",
      "- file_count: 4 bytes\n",
      "- created_at: 8 bytes\n",
      "- Total: ~56 bytes\n",
      "- 30K clusters: ~1.7 MB\n",
      "\n",
      "media_cluster_members:\n",
      "- cluster_id: 16 bytes\n",
      "- file_id: 16 bytes\n",
      "- hamming_distance: 4 bytes\n",
      "- Total: ~36 bytes\n",
      "- 300K memberships: ~10.8 MB\n",
      "\n",
      "Indexes (estimated 2.5x data size): ~1.1 GB\n",
      "\n",
      "Total Database: ~1.55 GB\n",
      "```\n",
      "\n",
      "#### Bloom Filter (In-Memory)\n",
      "\n",
      "```\n",
      "- 80 million bits for 1M files\n",
      "- 0.1% false positive rate\n",
      "- Total: ~10 MB\n",
      "```\n",
      "\n",
      "#### Cache Storage (Redis)\n",
      "\n",
      "```\n",
      "Quick hash cache:\n",
      "- Cache 200K hot entries\n",
      "- ~150 bytes per entry\n",
      "- Total: ~30 MB\n",
      "\n",
      "Perceptual hash cache:\n",
      "- Cache 50K media lookups\n",
      "- ~100 bytes per entry\n",
      "- Total: ~5 MB\n",
      "\n",
      "Total Cache: ~35 MB\n",
      "```\n",
      "\n",
      "#### Total Storage Overhead: **~1.6 GB for 1M files**\n",
      "\n",
      "**Additional in-memory: ~10 MB (Bloom filter)**\n",
      "\n",
      "---\n",
      "\n",
      "## Comparison Summary\n",
      "\n",
      "| Metric | Approach A | Approach C |\n",
      "|--------|-----------|-----------|\n",
      "| Database Storage | 1.2 GB | 1.55 GB |\n",
      "| Cache Storage | 20 MB | 35 MB |\n",
      "| In-Memory Structures | Minimal | 10 MB (Bloom) |\n",
      "| **Total** | **~1.22 GB** | **~1.6 GB** |\n",
      "| Duplicate Check Speed | 5-10ms (DB query) | 0.1-5ms (tiered) |\n",
      "| False Positives | 0% | <0.1% |\n",
      "| Upload Skip Rate | Exact only | Exact + Similar |\n",
      "\n",
      "**Key Tradeoff:** Approach C uses ~30% more storage but provides **10-100x faster** duplicate detection for most cases through aggressive filtering.\n"
     ]
    }
   ],
   "source": [
    "message_history = nodes_to_message_history(nodes_2)\n",
    "\n",
    "prompt_expand = \"\"\"Take the TOP 2 approaches from your ranking.\n",
    "For each of these two approaches, provide a detailed implementation design:\n",
    "\n",
    "For each approach, specify:\n",
    "1. Data structures needed (what gets stored, indexed)\n",
    "2. Upload flow (step-by-step what happens when a file is uploaded)\n",
    "3. Query flow (how to check if a file is a duplicate)\n",
    "4. Storage requirements (rough estimate for 1 million files)\n",
    "\n",
    "Be specific with technical details.\"\"\"\n",
    "\n",
    "agent_run_3, nodes_3 = await run_agent(\n",
    "    agent, prompt_expand, message_history=message_history\n",
    ")\n",
    "\n",
    "assert agent_run_3 is not None and agent_run_3.result is not None\n",
    "print(\"\\nExpanded Implementations:\")\n",
    "print(agent_run_3.result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_step4",
   "metadata": {},
   "source": [
    "## Step 4: Final Evaluation and Selection\n",
    "\n",
    "With detailed implementations for the top approaches, perform a final evaluation.\n",
    "Consider edge cases, failure modes, and operational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = nodes_to_message_history(nodes_3)\n",
    "\n",
    "prompt_final = \"\"\"Now that you have detailed implementations for the top 2 approaches:\n",
    "\n",
    "1. Identify edge cases or failure modes for each approach\n",
    "2. Consider operational complexity (monitoring, debugging, maintenance)\n",
    "3. Think about how each handles the file size range (1KB to 5GB)\n",
    "4. Choose the final winner\n",
    "5. Explain why this approach is superior given the implementation details\n",
    "\n",
    "Provide a clear final recommendation with specific reasoning.\"\"\"\n",
    "\n",
    "agent_run_4, nodes_4 = await run_agent(\n",
    "    agent, prompt_final, message_history=message_history\n",
    ")\n",
    "\n",
    "assert agent_run_4 is not None and agent_run_4.result is not None\n",
    "print(\"\\nFinal Selection:\")\n",
    "print(agent_run_4.result.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_agentic_patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
