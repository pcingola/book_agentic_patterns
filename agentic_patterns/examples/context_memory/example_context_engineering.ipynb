{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Engineering\n",
    "\n",
    "Managing context effectively when tools return large results: dataframes, logs, API responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_patterns.core.agents import get_agent, run_agent\n",
    "from agentic_patterns.core.context.decorators import context_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools That Return Large Data\n",
    "\n",
    "Tools that query databases or read logs often return thousands of rows. Without context management, this overwhelms the model's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sales_data(num_rows: int = 500) -> str:\n",
    "    \"\"\"Generate sample sales CSV data.\"\"\"\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    products = [\"Widget A\", \"Widget B\", \"Gadget X\", \"Gadget Y\", \"Device Z\"]\n",
    "    regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "    \n",
    "    lines = [\"date,product,region,quantity,price,total\"]\n",
    "    base_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        date = (base_date + timedelta(days=i % 365)).strftime(\"%Y-%m-%d\")\n",
    "        product = random.choice(products)\n",
    "        region = random.choice(regions)\n",
    "        qty = random.randint(1, 100)\n",
    "        price = round(random.uniform(10, 500), 2)\n",
    "        total = round(qty * price, 2)\n",
    "        lines.append(f\"{date},{product},{region},{qty},{price},{total}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_log_data(num_lines: int = 300) -> str:\n",
    "    \"\"\"Generate sample application log data.\"\"\"\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    levels = [\"INFO\", \"INFO\", \"INFO\", \"WARN\", \"ERROR\"]\n",
    "    components = [\"auth\", \"api\", \"db\", \"cache\", \"worker\"]\n",
    "    messages = {\n",
    "        \"INFO\": [\"Request processed successfully\", \"User logged in\", \"Cache hit\", \"Task completed\"],\n",
    "        \"WARN\": [\"Slow query detected\", \"High memory usage\", \"Rate limit approaching\"],\n",
    "        \"ERROR\": [\"Connection timeout\", \"Database error\", \"Authentication failed\", \"Invalid request\"]\n",
    "    }\n",
    "    \n",
    "    lines = []\n",
    "    base_time = datetime(2024, 6, 15, 10, 0, 0)\n",
    "    \n",
    "    for i in range(num_lines):\n",
    "        ts = (base_time + timedelta(seconds=i * 2)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        level = random.choice(levels)\n",
    "        component = random.choice(components)\n",
    "        msg = random.choice(messages[level])\n",
    "        lines.append(f\"[{ts}] [{level}] [{component}] {msg}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem: Large Results Consume Context\n",
    "\n",
    "Without truncation, tool results fill the context window. The model receives all data but may struggle to extract insights (\"the dumb zone\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sales_raw() -> str:\n",
    "    \"\"\"Query sales data without context management.\"\"\"\n",
    "    return generate_sales_data(500)\n",
    "\n",
    "# Check the size of raw data\n",
    "raw_data = query_sales_raw()\n",
    "print(f\"Raw data: {len(raw_data)} characters, {len(raw_data.split(chr(10)))} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: The @context_result Decorator\n",
    "\n",
    "The decorator truncates large results, keeping head and tail rows. The model sees a manageable preview while the full data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@context_result()\n",
    "def query_sales(ctx=None) -> str:\n",
    "    \"\"\"Query sales data with automatic truncation.\"\"\"\n",
    "    return generate_sales_data(500)\n",
    "\n",
    "@context_result()\n",
    "def search_logs(keyword: str, ctx=None) -> str:\n",
    "    \"\"\"Search application logs for a keyword.\"\"\"\n",
    "    logs = generate_log_data(300)\n",
    "    matching = [line for line in logs.split(\"\\n\") if keyword.upper() in line.upper()]\n",
    "    return \"\\n\".join(matching) if matching else \"No matches found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorated function returns truncated preview\n",
    "truncated = query_sales()\n",
    "print(f\"Truncated result: {len(truncated)} characters\")\n",
    "print(\"---\")\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent with Context-Managed Tools\n",
    "\n",
    "The agent receives truncated previews instead of full datasets, keeping context usage efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a data analyst assistant. When analyzing data:\n",
    "1. Summarize the structure and content from the preview\n",
    "2. Identify patterns or anomalies visible in the sample\n",
    "3. Note that full data is saved to the indicated path for detailed analysis\"\"\"\n",
    "\n",
    "agent = get_agent(system_prompt=system_prompt, tools=[query_sales, search_logs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Query the sales data and summarize what you see.\"\n",
    "result, _ = await run_agent(agent, prompt, verbose=True)\n",
    "print(\"\\n--- Agent Response ---\")\n",
    "print(result.result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Search the logs for ERROR entries and summarize the issues.\"\n",
    "result, _ = await run_agent(agent, prompt, verbose=True)\n",
    "print(\"\\n--- Agent Response ---\")\n",
    "print(result.result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History Compaction for Long Conversations\n",
    "\n",
    "Over many turns, conversation history accumulates. `HistoryCompactor` summarizes older exchanges when approaching context limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_patterns.core.context.history import HistoryCompactor, CompactionConfig\n",
    "from agentic_patterns.core.agents.utils import nodes_to_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure with low thresholds for demonstration\n",
    "config = CompactionConfig(max_tokens=500, target_tokens=200)\n",
    "\n",
    "async def log_compaction(result):\n",
    "    print(f\"  [COMPACTION] {result.original_messages} msgs ({result.original_tokens} tokens) -> {result.compacted_messages} msgs ({result.compacted_tokens} tokens)\")\n",
    "\n",
    "compactor = HistoryCompactor(config=config, on_compaction=log_compaction)\n",
    "\n",
    "print(f\"Compaction triggers at: {config.max_tokens} tokens\")\n",
    "print(f\"Targets reduction to: {config.target_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with history processor\n",
    "agent_with_compaction = get_agent(\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    history_processors=[compactor.create_history_processor()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Debug helper to show message contents\ndef show_messages(label: str, messages: list):\n    if not messages:\n        print(f\"  {label}: (empty)\")\n        return\n    print(f\"  {label}: {len(messages)} messages\")\n    for i, msg in enumerate(messages):\n        parts_summary = []\n        for part in msg.parts:\n            part_type = type(part).__name__\n            if hasattr(part, 'content'):\n                content = str(part.content)[:50] + \"...\" if len(str(part.content)) > 50 else str(part.content)\n                parts_summary.append(f\"{part_type}({content})\")\n            else:\n                parts_summary.append(part_type)\n        print(f\"    [{i}] {type(msg).__name__}: {', '.join(parts_summary)}\")\n\n# Simulate multi-turn conversation\nprompts = [\n    \"Explain what microservices architecture is.\",\n    \"What are the main benefits?\",\n    \"What are common challenges?\",\n    \"How does it compare to monolithic architecture?\",\n]\n\nmessage_history = None\nfor i, prompt in enumerate(prompts, 1):\n    print(f\"\\n{'='*60}\")\n    print(f\"TURN {i}: prompt = '{prompt}'\")\n    print(f\"{'='*60}\")\n    \n    show_messages(\"BEFORE run_agent (message_history)\", message_history)\n    \n    result, nodes = await run_agent(agent_with_compaction, prompt, message_history=message_history)\n    \n    print(f\"\\n  result.new_messages():\")\n    show_messages(\"new_messages\", result.new_messages())\n    \n    print(f\"\\n  compactor._last_processed_messages:\")\n    show_messages(\"_last_processed\", compactor._last_processed_messages)\n    \n    message_history = compactor.get_history_for_next_turn(result.new_messages())\n    \n    print(f\"\\n  AFTER get_history_for_next_turn:\")\n    show_messages(\"next message_history\", message_history)\n    \n    print(f\"\\n  Response: {result.result.output[:80]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_agentic_patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}