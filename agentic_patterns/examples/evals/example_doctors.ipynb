{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Doctors: AI-Powered Quality Analysis\n",
    "\n",
    "Doctors analyze AI artifacts (prompts, tools, MCP servers, agent cards, and skills) and provide recommendations.\n",
    "They use an LLM to evaluate quality, clarity, and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from agentic_patterns.core.doctors.prompt_doctor import PromptDoctor\n",
    "from agentic_patterns.core.doctors.tool_doctor import ToolDoctor\n",
    "from agentic_patterns.core.doctors.mcp_doctor import MCPDoctor\n",
    "from agentic_patterns.core.doctors.a2a_doctor import A2ADoctor\n",
    "from agentic_patterns.core.doctors.skill_doctor import SkillDoctor\n",
    "from pydantic_ai.mcp import MCPServerStdio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-doctor-section",
   "metadata": {},
   "source": [
    "## PromptDoctor: Analyzing Prompts\n",
    "\n",
    "PromptDoctor evaluates prompt templates for clarity, completeness, and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-doctor-bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A poorly written prompt\n",
    "bad_prompt = \"do the thing with {x}\"\n",
    "\n",
    "doctor = PromptDoctor()\n",
    "result = await doctor.analyze(bad_prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-doctor-good",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A well-written prompt\n",
    "good_prompt = \"\"\"You are a technical writer. Summarize the following document in 3 bullet points.\n",
    "\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Respond with exactly 3 bullet points, each starting with a dash.\"\"\"\n",
    "\n",
    "result = await doctor.analyze(good_prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool-doctor-section",
   "metadata": {},
   "source": [
    "## ToolDoctor: Analyzing Tool Functions\n",
    "\n",
    "ToolDoctor evaluates tool definitions for proper naming, documentation, and type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-doctor-bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A poorly defined tool\n",
    "def do_stuff(x):\n",
    "    \"\"\"Does stuff.\"\"\"\n",
    "    return x\n",
    "\n",
    "\n",
    "doctor = ToolDoctor()\n",
    "result = await doctor.analyze(do_stuff)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-doctor-good",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A well-defined tool\n",
    "def search_database(query: str, limit: int = 10) -> list[dict]:\n",
    "    \"\"\"Search the database for records matching the query.\n",
    "\n",
    "    Returns a list of matching records, each containing 'id', 'name', and 'score' fields.\n",
    "    \"\"\"\n",
    "    return []\n",
    "\n",
    "\n",
    "result = await doctor.analyze(search_database)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-doctor-section",
   "metadata": {},
   "source": [
    "## MCPDoctor: Analyzing MCP Server Tools\n",
    "\n",
    "MCPDoctor connects to an MCP server and analyzes all its exposed tools.\n",
    "We have two example servers: `mcp_server_bad.py` (poorly defined) and `mcp_server_good.py` (well defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcp-doctor-bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the poorly defined MCP server\n",
    "mcp_server = MCPServerStdio(\n",
    "    command=\"fastmcp\", args=[\"run\", \"-t\", \"stdio\", \"mcp_server_bad.py\"]\n",
    ")\n",
    "\n",
    "doctor = MCPDoctor(mcp_server)\n",
    "results = await doctor.analyze_all()\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcp-doctor-good",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the well-defined MCP server\n",
    "mcp_server = MCPServerStdio(\n",
    "    command=\"fastmcp\", args=[\"run\", \"-t\", \"stdio\", \"mcp_server_good.py\"]\n",
    ")\n",
    "\n",
    "doctor = MCPDoctor(mcp_server)\n",
    "results = await doctor.analyze_all()\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a-doctor-section",
   "metadata": {},
   "source": [
    "## A2ADoctor: Analyzing Agent Cards\n",
    "\n",
    "A2ADoctor connects to A2A servers and analyzes their agent cards.\n",
    "\n",
    "Start the servers in separate terminals:\n",
    "\n",
    "```bash\n",
    "# In termina 1\n",
    "cd agentic_patterns/examples/evals\n",
    "uvicorn a2a_server_bad:app --port 8001\n",
    "\n",
    "# In termina 2\n",
    "cd agentic_patterns/examples/evals\n",
    "uvicorn a2a_server_good:app --port 8002\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a-doctor-bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the poorly defined A2A server\n",
    "doctor = A2ADoctor()\n",
    "result = await doctor.analyze(\"http://127.0.0.1:8001\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a-doctor-good",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the well-defined A2A server\n",
    "result = await doctor.analyze(\"http://127.0.0.1:8002\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zykzv0igfb",
   "metadata": {},
   "source": [
    "## SkillDoctor: Analyzing Agent Skills\n",
    "\n",
    "SkillDoctor analyzes Agent Skills (agentskills.io format) for compliance and quality.\n",
    "It validates the directory structure, SKILL.md frontmatter, body content, and script documentation.\n",
    "\n",
    "We have two example skills: `skill-bad/` (poorly defined) and `skill-good/` (well defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j56k6of0hxh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the poorly defined skill\n",
    "skill_dir = Path(\"skill-bad\")\n",
    "\n",
    "doctor = SkillDoctor()\n",
    "result = await doctor.analyze(skill_dir)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6qpcia3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the well-defined skill\n",
    "skill_dir = Path(\"skill-good\")\n",
    "\n",
    "doctor = SkillDoctor()\n",
    "result = await doctor.analyze(skill_dir)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d001567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_agentic_patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
