{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Eval Runner and Custom Evaluators\n",
    "\n",
    "The eval runner discovers `eval_*.py` files, pairs datasets with target functions,\n",
    "and produces structured reports. This notebook demonstrates discovery, execution,\n",
    "and the custom evaluators provided by the core library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from agentic_patterns.core.evals import (\n",
    "    discover_datasets,\n",
    "    find_eval_files,\n",
    "    run_evaluation,\n",
    "    PrintOptions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discovery-section",
   "metadata": {},
   "source": [
    "## Discovery\n",
    "\n",
    "The runner scans a directory for `eval_*.py` files, then discovers\n",
    "`dataset_*` objects, `target_*` functions, and optional `scorer_*` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_dir = Path(\".\")\n",
    "eval_files = find_eval_files(evals_dir)\n",
    "print(f\"Found eval files: {[f.name for f in eval_files]}\")\n",
    "\n",
    "datasets = discover_datasets(eval_files, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-section",
   "metadata": {},
   "source": [
    "## Running an Evaluation\n",
    "\n",
    "Each discovered dataset is run against its target function.\n",
    "The report shows per-case results and aggregate scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_options = PrintOptions(\n",
    "    include_input=True,\n",
    "    include_output=True,\n",
    "    include_reasons=True,\n",
    ")\n",
    "\n",
    "for ds in datasets:\n",
    "    name, success, report = await run_evaluation(ds, print_options, verbose=True)\n",
    "    print(f\"\\nResult: {'PASSED' if success else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-evaluators-section",
   "metadata": {},
   "source": [
    "## Custom Evaluators\n",
    "\n",
    "The core library provides four agent-specific evaluators that go beyond\n",
    "basic string or type checks. They can be used in any dataset alongside\n",
    "the built-in pydantic-evals evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-evaluators",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_evals import Case, Dataset\n",
    "\n",
    "from agentic_patterns.core.evals import (\n",
    "    OutputContainsJson,\n",
    "    OutputMatchesSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityInfo(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "\n",
    "dataset_schema = Dataset(\n",
    "    cases=[\n",
    "        Case(\n",
    "            name=\"valid_json\",\n",
    "            inputs=\"valid\",\n",
    "            expected_output='{\"city\": \"Paris\", \"country\": \"France\"}',\n",
    "        ),\n",
    "        Case(\n",
    "            name=\"invalid_json\",\n",
    "            inputs=\"invalid\",\n",
    "            expected_output=\"not json at all\",\n",
    "        ),\n",
    "    ],\n",
    "    evaluators=[\n",
    "        OutputContainsJson(),\n",
    "        OutputMatchesSchema(schema=CityInfo),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "async def passthrough(inp: str) -> str:\n",
    "    \"\"\"Return the expected output as-is for demonstration.\"\"\"\n",
    "    # In a real eval, this would call an agent\n",
    "    case = [c for c in dataset_schema.cases if c.inputs == inp][0]\n",
    "    return case.expected_output\n",
    "\n",
    "\n",
    "report = await dataset_schema.evaluate(passthrough)\n",
    "report.print(include_input=True, include_output=True, include_reasons=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cli-section",
   "metadata": {},
   "source": [
    "## CLI Usage\n",
    "\n",
    "The same discovery and execution logic is available from the command line:\n",
    "\n",
    "```bash\n",
    "# Run all eval_*.py files in a directory\n",
    "python -m agentic_patterns.core.evals --evals-dir agentic_patterns/examples/evals --verbose\n",
    "\n",
    "# Filter to a specific dataset\n",
    "python -m agentic_patterns.core.evals --evals-dir agentic_patterns/examples/evals --filter capitals\n",
    "```\n",
    "\n",
    "The CLI returns a non-zero exit code when any evaluation fails, making it suitable as a CI gate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
