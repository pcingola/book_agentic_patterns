{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72370a6c",
   "metadata": {},
   "source": [
    "# Pydantic Evals Framework\n",
    "\n",
    "A structured approach to evals using pydantic-evals library.\n",
    "Core abstractions: Case (test scenario), Evaluator (what \"good\" means), Dataset (collection of cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc48c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import format_as_xml\n",
    "from pydantic_evals import Case, Dataset\n",
    "from pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance, LLMJudge\n",
    "\n",
    "from agentic_patterns.core.agents import get_agent, run_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3790c9",
   "metadata": {},
   "source": [
    "## Core Concepts: Case, Evaluator, Dataset\n",
    "\n",
    "A Case defines inputs, expected outputs, and metadata for a single test scenario.\n",
    "An Evaluator checks whether the output meets specific criteria.\n",
    "A Dataset combines cases with evaluators and runs them against a task function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a10ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "case1 = Case(\n",
    "    name='simple_case',\n",
    "    inputs='What is the capital of France?',\n",
    "    expected_output='Paris',\n",
    "    metadata={'difficulty': 'easy'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d466604",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MyEvaluator(Evaluator):\n",
    "    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:  \n",
    "        if ctx.output == ctx.expected_output:\n",
    "            return 1.0\n",
    "        elif (\n",
    "            isinstance(ctx.output, str)\n",
    "            and ctx.expected_output.lower() in ctx.output.lower()\n",
    "        ):\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    cases=[case1],\n",
    "    evaluators=[IsInstance(type_name='str'), MyEvaluator()],  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d667ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def guess_city(question: str) -> str:  \n",
    "    return 'Paris'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c98f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = await dataset.evaluate(guess_city)  \n",
    "report.print(include_input=True, include_output=True, include_durations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392c277",
   "metadata": {},
   "source": [
    "## LLM Judge with Real Agent\n",
    "\n",
    "LLMJudge evaluates open-ended outputs against a rubric.\n",
    "Here we test a recipe agent: per-case evaluators check dietary constraints,\n",
    "global evaluators check general quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerOrder(BaseModel):  \n",
    "    dish_name: str\n",
    "    dietary_restriction: str | None = None\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str]\n",
    "    steps: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c57cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_agent = get_agent(\n",
    "    output_type=Recipe,\n",
    "    system_prompt='Generate a recipe to cook the dish that meets the dietary restrictions.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def transform_recipe(customer_order: CustomerOrder) -> Recipe:  \n",
    "    res, nodes = await run_agent(recipe_agent, format_as_xml(customer_order), verbose=True)\n",
    "    return res # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f7c48",
   "metadata": {},
   "outputs": [],
   "source": "model = recipe_agent.model\n\nrecipe_dataset = Dataset[CustomerOrder, Recipe, Any](  \n    cases=[\n        Case(\n            name='vegetarian_recipe',\n            inputs=CustomerOrder(dish_name='Spaghetti Bolognese', dietary_restriction='vegetarian'),\n            expected_output=None,  # \n            metadata={'focus': 'vegetarian'},\n            evaluators=(\n                LLMJudge(  \n                    rubric='Recipe should not contain meat or fish',\n                    model=model\n                ),\n            ),\n        ),\n        Case(\n            name='gluten_free_recipe',\n            inputs=CustomerOrder(dish_name='Chocolate Cake', dietary_restriction='gluten-free'),\n            expected_output=None,\n            metadata={'focus': 'gluten-free'},\n            # Case-specific evaluator with a focused rubric\n            evaluators=(\n                LLMJudge(\n                    rubric='Recipe should not contain gluten or wheat products',\n                    model=model\n                ),\n            ),\n        ),\n    ],\n    evaluators=[  \n        IsInstance(type_name='Recipe'),\n        LLMJudge(\n            rubric='Recipe should have clear steps and relevant ingredients',\n            include_input=True,\n            model=model,\n        ),\n    ],\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = await recipe_dataset.evaluate(transform_recipe)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_agentic_patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}