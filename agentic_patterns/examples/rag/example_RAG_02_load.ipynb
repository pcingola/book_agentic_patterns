{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RAG: Load Vector Database (LLM-based Chunking)\n",
    "\n",
    "This notebook loads book chapters into a Chroma vector database using LLM-based semantic chunking.\n",
    "Instead of naive paragraph splitting, an LLM analyzes each chapter and chunks it by topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-header",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from agentic_patterns.core.config.config import MAIN_PROJECT_DIR\n",
    "from agentic_patterns.core.agents import get_agent, run_agent\n",
    "from agentic_patterns.core.vectordb import get_vector_db, vdb_add, load_vectordb_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = MAIN_PROJECT_DIR / 'tests' / 'data' / 'books'\n",
    "COLLECTION_NAME = 'books_llm_chunked'\n",
    "print(f\"Books directory: {DOCS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vdb-header",
   "metadata": {},
   "source": [
    "## Vector-db: Setup\n",
    "\n",
    "Creates/loads a Chroma vector database collection. Uses a separate collection name to distinguish from naive chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vdb-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb = get_vector_db(COLLECTION_NAME)\n",
    "\n",
    "settings = load_vectordb_settings(MAIN_PROJECT_DIR / \"config.yaml\")\n",
    "db_path = Path(settings.get_vectordb().persist_directory)\n",
    "print(f\"Database directory: {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-populate",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = vdb.count()\n",
    "create_vdb = (count == 0)\n",
    "print(f\"Collection has {count} documents. Need to populate: {create_vdb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-header",
   "metadata": {},
   "source": [
    "## LLM-based Chunking\n",
    "\n",
    "Instead of splitting text at fixed character boundaries or by paragraphs, we use an LLM to identify semantic boundaries. The LLM reads the text and groups sentences that belong to the same topic, scene, or theme.\n",
    "\n",
    "The challenge is that large documents exceed the LLM's context window or become slow to process. Our solution processes text in batches while preserving semantic coherence across batch boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunking-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKING_PROMPT = \"\"\"\n",
    "You are a text chunking assistant. Your task is to divide the following text into coherent chunks based on topics or themes.\n",
    "\n",
    "Guidelines:\n",
    "- Each chunk should be self-contained and focus on a single topic, scene, or theme\n",
    "- Chunks should be substantial (at least a few sentences) but not too long\n",
    "- Preserve the original text exactly - do not summarize or modify the content\n",
    "- Return the chunks as a list of strings\n",
    "- IMPORTANT: If the text ends mid-topic (incomplete), include that partial content as the LAST chunk so it can be continued in the next batch\n",
    "\n",
    "TEXT TO CHUNK:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "chunking_agent = get_agent(config_name=\"fast\", output_type=list[str])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnbq9zjmnd",
   "metadata": {},
   "source": [
    "### Batching Strategy\n",
    "\n",
    "We split the document into batches of approximately 15000 characters (~3000-4000 tokens). Rather than cutting at arbitrary positions, we split at paragraph boundaries (double newlines) to avoid breaking sentences. Each batch contains complete paragraphs that fit within the size limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bq5x8arkhso",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_CHARS = 15000\n",
    "\n",
    "def split_into_batches(text: str, batch_size: int) -> list[str]:\n",
    "    \"\"\"Split text into batches by paragraphs, respecting batch_size limit.\"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_size = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_size = len(para) + 2  # +2 for the \\n\\n separator\n",
    "        if current_size + para_size > batch_size and current_batch:\n",
    "            # Current batch is full, start a new one\n",
    "            batches.append('\\n\\n'.join(current_batch))\n",
    "            current_batch = [para]\n",
    "            current_size = para_size\n",
    "        else:\n",
    "            current_batch.append(para)\n",
    "            current_size += para_size\n",
    "    \n",
    "    # Don't forget the last batch\n",
    "    if current_batch:\n",
    "        batches.append('\\n\\n'.join(current_batch))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mb2fsa7gasp",
   "metadata": {},
   "source": [
    "### Handling Incomplete Chunks (Leftover Logic)\n",
    "\n",
    "When we split text into batches, a batch boundary might fall in the middle of a semantic topic. For example, batch 1 might end with the beginning of a conversation, and batch 2 continues it.\n",
    "\n",
    "To handle this, we use a \"leftover\" strategy: the last chunk returned by the LLM for each batch (except the final batch) is assumed to be potentially incomplete. We remove it from the results and prepend it to the next batch. This way, the LLM sees the incomplete content again with additional context and can properly finish chunking it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chunk_with_llm(file: Path) -> list[tuple[str, str, dict]]:\n",
    "    \"\"\"Chunk a file using LLM-based semantic chunking with batching.\"\"\"\n",
    "    text = file.read_text()\n",
    "    batches = split_into_batches(text, BATCH_SIZE_CHARS)\n",
    "    print(f\"  Split into {len(batches)} batches\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    leftover = \"\"\n",
    "    \n",
    "    for batch_num, batch in enumerate(batches):\n",
    "        # Prepend leftover from previous batch\n",
    "        batch_text = leftover + batch if leftover else batch\n",
    "        leftover = \"\"\n",
    "        \n",
    "        prompt = CHUNKING_PROMPT.format(text=batch_text)\n",
    "        agent_run, _ = await run_agent(chunking_agent, prompt, verbose=False)\n",
    "        chunks: list[str] = agent_run.result.output\n",
    "        \n",
    "        if not chunks:\n",
    "            continue\n",
    "        \n",
    "        # Last chunk might be incomplete - save as leftover for next batch\n",
    "        if batch_num < len(batches) - 1 and chunks:\n",
    "            leftover = chunks.pop()\n",
    "        \n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"    Batch {batch_num + 1}/{len(batches)}: {len(chunks)} chunks\")\n",
    "    \n",
    "    # Add final leftover as the last chunk\n",
    "    if leftover:\n",
    "        all_chunks.append(leftover)\n",
    "    \n",
    "    # Format results with IDs and metadata\n",
    "    results = []\n",
    "    for chunk_num, chunk in enumerate(all_chunks):\n",
    "        doc = chunk.strip()\n",
    "        if not doc:\n",
    "            continue\n",
    "        doc_id = f\"{file.stem}-llm-{chunk_num}\"\n",
    "        metadata = {'source': str(file.stem), 'chunk': chunk_num, 'method': 'llm'}\n",
    "        results.append((doc, doc_id, metadata))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_vdb:\n",
    "    count_added = 0\n",
    "    for txt_file in DOCS_DIR.glob('*.txt'):\n",
    "        print(f\"Processing file '{txt_file.name}' with LLM chunking...\")\n",
    "        chunks = await chunk_with_llm(txt_file)\n",
    "        print(f\"  LLM produced {len(chunks)} chunks\")\n",
    "        \n",
    "        for doc, doc_id, meta in chunks:\n",
    "            vdb_add(vdb, text=doc, doc_id=doc_id, meta=meta)\n",
    "            print(f\"  Added doc_id: {doc_id}\")\n",
    "            count_added += 1\n",
    "    \n",
    "    print(f\"\\nTotal documents added: {count_added}\")\n",
    "    assert count_added > 0, f\"No documents added. Check books directory: {DOCS_DIR}\"\n",
    "else:\n",
    "    print(\"Database already populated, skipping load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final document count: {vdb.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_agentic_patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
