{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RAG: Load Vector Database (LLM-based Chunking)\n",
    "\n",
    "This notebook loads book chapters into a Chroma vector database using LLM-based semantic chunking.\n",
    "Instead of naive paragraph splitting, an LLM analyzes each chapter and chunks it by topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-header",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from agentic_patterns.core.config.config import MAIN_PROJECT_DIR\n",
    "from agentic_patterns.core.agents import get_agent, run_agent\n",
    "from agentic_patterns.core.vectordb import get_vector_db, vdb_add, load_vectordb_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = MAIN_PROJECT_DIR / 'tests' / 'data' / 'books'\n",
    "COLLECTION_NAME = 'books_llm_chunked'\n",
    "print(f\"Books directory: {DOCS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vdb-header",
   "metadata": {},
   "source": [
    "## Vector-db: Setup\n",
    "\n",
    "Creates/loads a Chroma vector database collection. Uses a separate collection name to distinguish from naive chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vdb-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb = get_vector_db(COLLECTION_NAME)\n",
    "\n",
    "settings = load_vectordb_settings(MAIN_PROJECT_DIR / \"config.yaml\")\n",
    "db_path = Path(settings.get_vectordb().persist_directory)\n",
    "print(f\"Database directory: {db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-populate",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = vdb.count()\n",
    "create_vdb = (count == 0)\n",
    "print(f\"Collection has {count} documents. Need to populate: {create_vdb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-header",
   "metadata": {},
   "source": [
    "## LLM-based Chunking\n",
    "\n",
    "Uses an LLM to analyze the text and split it into semantically coherent chunks based on topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunking-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKING_PROMPT = \"\"\"\n",
    "You are a text chunking assistant. Your task is to divide the following text into coherent chunks based on topics or themes.\n",
    "\n",
    "Guidelines:\n",
    "- Each chunk should be self-contained and focus on a single topic, scene, or theme\n",
    "- Chunks should be substantial (at least a few sentences) but not too long\n",
    "- Preserve the original text exactly - do not summarize or modify the content\n",
    "- Return the chunks as a list of strings\n",
    "\n",
    "TEXT TO CHUNK:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "chunking_agent = get_agent(output_type=list[str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunk-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chunk_with_llm(file: Path) -> list[tuple[str, str, dict]]:\n",
    "    \"\"\"Chunk a file using LLM-based semantic chunking.\"\"\"\n",
    "    text = file.read_text()\n",
    "    prompt = CHUNKING_PROMPT.format(text=text)\n",
    "    \n",
    "    agent_run, _ = await run_agent(chunking_agent, prompt, verbose=True)\n",
    "    chunks: list[str] = agent_run.result.output\n",
    "    \n",
    "    results = []\n",
    "    for chunk_num, chunk in enumerate(chunks):\n",
    "        doc = chunk.strip()\n",
    "        if not doc:\n",
    "            continue\n",
    "        doc_id = f\"{file.stem}-llm-{chunk_num}\"\n",
    "        metadata = {'source': str(file.stem), 'chunk': chunk_num, 'method': 'llm'}\n",
    "        results.append((doc, doc_id, metadata))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_vdb:\n",
    "    count_added = 0\n",
    "    for txt_file in DOCS_DIR.glob('*.txt'):\n",
    "        print(f\"Processing file '{txt_file.name}' with LLM chunking...\")\n",
    "        chunks = await chunk_with_llm(txt_file)\n",
    "        print(f\"  LLM produced {len(chunks)} chunks\")\n",
    "        \n",
    "        for doc, doc_id, meta in chunks:\n",
    "            vdb_add(vdb, text=doc, doc_id=doc_id, meta=meta)\n",
    "            print(f\"  Added doc_id: {doc_id}\")\n",
    "            count_added += 1\n",
    "    \n",
    "    print(f\"\\nTotal documents added: {count_added}\")\n",
    "    assert count_added > 0, f\"No documents added. Check books directory: {DOCS_DIR}\"\n",
    "else:\n",
    "    print(\"Database already populated, skipping load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final document count: {vdb.count()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
