## Historical Perspectives

The Foundations chapter established that modern agentic systems preserve the recursive decide-act structure formalized by Bellman and classical reinforcement learning, replacing explicit value functions with language models that approximate policies through natural language reasoning. This chapter traces how the specific reasoning patterns that make that approximation practical -- prompting, chain-of-thought, search, reflection, and verification -- converged over a remarkably short period, roughly 2020 to 2023, as large pre-trained models revealed capabilities that decades of earlier research had pursued through very different means. Understanding this convergence clarifies why the patterns take the forms they do and why they appeared in the order they did.

The groundwork was laid by transfer learning and representation learning in the late 2000s and 2010s. Research in computer vision and natural language processing demonstrated that models trained on large, general datasets could transfer useful representations to downstream tasks, including categories never seen during training. In NLP, distributed word representations showed that semantic structure learned from large corpora could generalize beyond explicit supervision. These results established a principle that would later prove central to agentic reasoning: general-purpose representations, trained at scale, can substitute for task-specific engineering.

The decisive shift came with large pre-trained language models. GPT-2 (2019) and especially GPT-3 (2020) demonstrated *in-context learning*: models could infer a task from examples and instructions embedded in the prompt, without any parameter updates. The GPT-3 paper formalized zero-shot, one-shot, and few-shot prompting, reframing few-shot learning from a training paradigm into a prompting paradigm. This reframing had immediate consequences for agent design. If behavior could be shaped at inference time through careful prompt construction, then flexible, task-adaptive agents no longer required retraining or hand-coded rules. Meta-learning research later connected these behaviors to implicit task induction and probabilistic sequence modeling, suggesting that models infer latent task descriptions from prompts and condition their generation accordingly.

Once in-context learning was established, the next question was whether models could reason, not merely pattern-match. The answer came in 2022 with Chain-of-Thought prompting. Researchers observed that large models often failed at multi-step reasoning despite strong surface-level performance, but could be induced to succeed by generating intermediate reasoning steps. The key insight, rooted in long-standing ideas from cognitive science and educational psychology about the value of "showing your work," was that externalizing intermediate steps transformed models from black-box answer generators into systems producing inspectable reasoning traces. Chain-of-Thought was the watershed moment for agentic reasoning: it proved that the *structure* of model output, not just its content, could be engineered to improve capability.

From Chain-of-Thought, several lines of development radiated outward almost simultaneously. One direction addressed the limitation that linear reasoning commits to a single trajectory. Drawing on classical AI techniques of tree search, planning, and heuristic evaluation, researchers in 2023 formalized Tree of Thought, which treats reasoning as a process of branching, scoring, and pruning alternative paths rather than following a single sequence. This reconnected language model reasoning with decades of work in combinatorial search and planning, from STRIPS and hierarchical task networks in the 1960s-70s to modern automated planning, while replacing formal world models with natural language state descriptions.

A second direction grounded reasoning in action. Chain-of-Thought improved decomposition and planning but remained "closed-book," forcing models to reason purely in-token and leaving them vulnerable to hallucination when external information was needed. Multiple threads converged on fixing this: WebGPT (late 2021) trained models to browse the web while collecting citations, MRKL systems (mid 2022) articulated modular neuro-symbolic architectures routing subproblems to specialized tools, and SayCan explored selecting feasible actions through affordance models. ReAct (October 2022) crystallized these threads into a general prompt format that interleaves reasoning traces with tool calls and observations, creating a closed loop between thought and environment. Shortly after, CodeAct (2023) pushed this further by making executable code the primary reasoning modality rather than natural language, connecting to earlier work on program synthesis and neural program induction.

A third direction turned reasoning inward. Cognitive science had long studied metacognition, the ability to monitor and correct one's own reasoning, and classical AI had implemented related ideas in planning systems with execution monitoring and belief revision. In the LLM context, researchers observed that models could improve their own outputs when prompted to critique or reconsider them. Papers such as *Self-Refine* and *Reflexion* formalized explicit generate-reflect-revise loops. Closely related, work on self-consistency and verifier models separated generation from validation, echoing the classical AI distinction between search and verification and drawing additional impetus from alignment research on identifying harmful or incorrect outputs.

Running through all of these developments was a renewed emphasis on human oversight. The idea of keeping humans involved in automated decision-making dates back to supervisory control research in the 1960s and active learning in the 2000s, but it re-emerged as a practical necessity when autonomous LLM agents demonstrated both impressive capabilities and alarming failure modes: hallucinations, unsafe actions, and irreversible side effects from tool use. Human-in-the-loop patterns shifted from a theoretical preference to a core design principle for any agent operating in real-world, safety-critical, or compliance-sensitive environments.

By 2023, all of these patterns were available simultaneously, forming a toolkit that could be composed rather than applied in isolation. Zero-shot and few-shot prompting provide the adaptive baseline. Chain-of-Thought makes reasoning explicit and inspectable. Tree of Thought extends it into search. ReAct and CodeAct ground reasoning in action and execution. Self-reflection and verification provide corrective feedback. Planning connects to classical decomposition. Human-in-the-loop ensures safety at critical boundaries. What is remarkable about this convergence is its speed: techniques that individually drew on decades of research in cognitive science, planning, program synthesis, information retrieval, and control theory all became practical within a span of roughly three years, enabled by the same underlying capability -- large-scale pre-trained models that can follow structured instructions at inference time.
