## Historical Perspective

Software testing as a routine engineering practice is remarkably recent. For most of the history of programming, from the 1950s through the 1990s, what passed for testing was indistinguishable from debugging: developers ran their programs on a handful of example inputs, observed whether outputs looked correct, and fixed problems as they surfaced. There was no concept of systematic test suites, automated validation, or repeatable regression checks. Programs were small, bespoke, and tightly coupled to specific hardware, so the cost of informal validation seemed acceptable.

Academic and methodological discussions of testing did emerge during the 1970s and 1980s. Researchers and authors wrote about test planning, test case design, and the distinction between demonstrating that software works versus revealing that it contains defects. These ideas appeared in textbooks and conference papers, but they had little impact on day-to-day industry practice. Most organizations relied on manual QA performed late in the development cycle, often by separate teams who exercised the system through realistic scenarios without any formalized structure. The notion of "unit testing" existed in principle, but without accessible frameworks or tooling, it remained a theoretical concept rather than something developers actually did.

The real transformation began only in the late 1990s and accelerated through the early 2000s. Several factors converged: object-oriented design made code more modular and testable, machines became fast enough that running thousands of tests was practical, and critically, usable testing frameworks finally appeared and gained adoption. JUnit, released in 1997, was among the first frameworks that made writing and running automated tests genuinely accessible. Its influence spread rapidly, spawning equivalents in other languages and establishing patterns that developers could actually adopt. Continuous integration practices reinforced the value of automated tests by making them part of the development workflow rather than an afterthought. From this point onward, layered testing (smoke, unit, integration, system, and stress tests) became a practical reality rather than an academic taxonomy. The core assumption remained determinism: given identical inputs and state, the system should behave identically, enabling strict assertions and reliable regression detection.

Machine learning systems disrupted this assumption without eliminating the need for testing. Models trained on data exhibit non-deterministic behavior, sensitivity to sampling, and performance that must be evaluated statistically. Testing expanded to include dataset-based validation, tolerance ranges, and aggregate metrics, while deterministic tests remained essential for data pipelines, feature extraction, and serving infrastructure. The testing discipline adapted, but its foundational techniques still applied to the deterministic components surrounding the model.

Agentic systems push this evolution further. An agent combines stochastic model reasoning with deterministic code, external tools, long-lived state, and multi-step workflows. Testing such systems requires a clear separation of concerns. Deterministic components, including tools, planners, routers, protocol handlers, and persistence layers, should be tested using classical techniques with strict assertions. Agent behavior, by contrast, cannot be validated through exact output matching. Instead, tests assert properties: that outputs conform to schemas, that required tools were invoked, that safety constraints were respected, or that workflows completed within defined bounds.

This distinction clarifies the relationship between testing, evals, and benchmarks. Testing establishes that the system is wired correctly and fails in predictable ways. Evals, introduced later in this chapter, focus on whether agents behave correctly and reliably under realistic tasks and prompts. Benchmarks probe capability limits and comparative performance, but they are not substitutes for correctness testing. In agentic systems, rigorous testing of deterministic layers is what makes higher-level eval results interpretable rather than noisy or misleading.

