## References

1. Tom B. Brown et al. *Language Models are Few-Shot Learners*. NeurIPS, 2020. https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
2. Jason Wei et al. *Finetuned Language Models Are Zero-Shot Learners*. ICLR, 2022. https://openreview.net/pdf?id=gEZrGCozdqR
3. Long Ouyang et al. *Training language models to follow instructions with human feedback*. NeurIPS, 2022. https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf
4. Ashish Vaswani et al. *Attention Is All You Need*. NeurIPS, 2017. https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
5. Sainbayar Sukhbaatar et al. *End-To-End Memory Networks*. NeurIPS, 2015. https://proceedings.neurips.cc/paper/5846-end-to-end-memory-networks.pdf
6. Mikael Henaff et al. *Tracking the World State with Recurrent Entity Networks*. ICLR, 2017. https://arxiv.org/pdf/1612.03969
7. Pydantic AI Documentation. *Agents: System Prompts, Instructions, Runs vs. Conversations*. 2025. https://ai.pydantic.dev/agents/
8. Ruirui Lou et al. *Large Language Model Instruction Following: A Survey of Progress and Challenges*. Computational Linguistics, 2024. https://direct.mit.edu/coli/article/50/3/1053/121669/Large-Language-Model-Instruction-Following-A
