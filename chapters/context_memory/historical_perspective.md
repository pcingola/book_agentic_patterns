## Historical Perspective

The Core Patterns chapter traced how in-context learning, chain-of-thought prompting, and related reasoning techniques converged between 2020 and 2023. That convergence had a less visible but equally consequential side effect: it turned the input to the model -- the prompt, the history, the retrieved documents, the tool outputs -- into the primary lever for controlling agent behavior. Once behavior could be shaped by what the model sees rather than by retraining or hand-coded rules, managing that input became a first-class engineering discipline. The historical roots of context and memory management lie in three threads that developed in parallel and converged as agentic systems matured.

The first thread is the evolution of prompts from informal strings into structured, layered interfaces. Instruction tuning, exemplified by FLAN ([2]), and reinforcement learning from human feedback, exemplified by InstructGPT ([3]), made models reliably follow explicit instructions. This separated the prompt into distinct functional layers -- system instructions, developer guidance, user input, and conversation history -- each with different persistence and update semantics. As agents accumulated tools, orchestration logic, and retrieved context, these layers became engineering contracts rather than ad-hoc text, requiring careful design to avoid conflicts and redundancy.

The second thread is explicit memory as a mechanism to maintain state across turns. Memory Networks ([4]) and related architectures used external or structured memory modules to retain and retrieve relevant facts during multi-step interactions; later work, such as recurrent entity-centric memories ([5]), focused on tracking evolving state as new text arrived. These ideas strongly influenced today's agent long-term memory patterns -- store, retrieve, ground, update -- even though most production systems now implement memory outside the model as databases and retrieval pipelines rather than as learned memory modules inside the network.

The third thread is the discovery that larger context windows do not automatically improve performance. As windows grew from thousands to tens and eventually hundreds of thousands of tokens, a practical realization emerged: models do not use arbitrarily long contexts uniformly or reliably. Relevance, ordering, redundancy, and information density matter more than sheer length. Developers found that indiscriminately adding text often degraded performance rather than improving it. This broadened the scope from crafting individual prompts to managing the entire context window as a constrained, shared resource that must accommodate instructions, conversation history, retrieved documents, tool outputs, and intermediate reasoning. The discipline evolved from prompt engineering to context management and ultimately to context engineering as a runtime systems concern -- one that encompasses not just what to say, but what to include, what to omit, how to structure information, and when to refresh it.

[2]: https://openreview.net/pdf?id=gEZrGCozdqR "Finetuned Language Models Are Zero-Shot Learners"
[3]: https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf "Training language models to follow instructions with human feedback"
[4]: https://proceedings.neurips.cc/paper/5846-end-to-end-memory-networks.pdf "End-To-End Memory Networks"
[5]: https://arxiv.org/pdf/1612.03969 "Tracking the World State with Recurrent Entity Networks"
