## Historical Perspective

The modern understanding of context and memory in agentic systems emerged from the convergence of several research threads between 2018 and 2022. The most visible turning point was the discovery of in-context learning: large transformer-based language models, culminating in GPT-3 in 2020, demonstrated that they could perform new tasks simply by conditioning on examples and instructions embedded in the input text, without any gradient updates. This shifted attention away from fine-tuning as the primary adaptation mechanism and toward prompts as a programmable interface to a fixed model. Careful prompt construction—selecting examples, phrasing instructions, ordering information—could substitute for task-specific training in many cases, a style that came to be known as "prompting as programming."

A second line of work refined this interface by making instructions explicit and reliable. Instruction tuning, exemplified by FLAN, and reinforcement learning from human feedback, exemplified by InstructGPT, reframed prompts from ad-hoc strings into a more structured contract: instructions that define what the model should do, separated from task input and conversational content. These advances made instruction adherence substantially more dependable, which in turn encouraged application developers to treat instructions as a first-class layer with clearer boundaries and evaluation strategies. Techniques such as chain-of-thought prompting further demonstrated that the structure of the input—not just its content—could significantly affect model reasoning, reinforcing the idea that prompts are engineering artifacts rather than casual requests.

In parallel, dialogue and reasoning systems explored explicit memory as a mechanism to maintain state across turns. Memory Networks and related architectures used external or structured memory modules to retain and retrieve relevant facts during multi-step interactions; later work, such as recurrent entity-centric memories, focused on tracking evolving state as new text arrived. These ideas strongly influenced today's agent long-term memory patterns—store, retrieve, ground, update—even though most production systems now implement memory outside the model as databases and retrieval pipelines rather than as learned memory modules inside the network.

As context windows grew from thousands to tens and eventually hundreds of thousands of tokens, a practical realization emerged: models do not use arbitrarily long contexts uniformly or reliably. Relevance, ordering, redundancy, and information density matter more than sheer length. Developers found that indiscriminately adding text often degraded performance rather than improving it. This broadened the scope from crafting individual prompts to managing the entire context window as a constrained, shared resource that must accommodate instructions, conversation history, retrieved documents, tool outputs, and intermediate reasoning. The discipline evolved from prompt engineering to context management and ultimately to context engineering as a runtime systems concern—one that encompasses not just what to say, but what to include, what to omit, how to structure information, and when to refresh it.
